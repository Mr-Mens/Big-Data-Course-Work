{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx9-Fre4FMda"
      },
      "source": [
        "# **Machine Learning on Big Data (CN7030) CRWK 23-24 Term B [60% weighting]**\n",
        "# **Group ID: [Group13]**\n",
        "1.   Student 1: Philip Acquaye-Mensah 2640756\n",
        "2.   Student 2: Mohamed Jareer MOHAMED  ZEENAM 2596353\n",
        "\n",
        "---\n",
        "\n",
        "If you want to add comments on your group work, please write it here for us:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdMZR-9QTwG3"
      },
      "source": [
        "\n",
        "# **Initiate and Configure Spark**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "_z0p88Xtw_3-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/16 23:11:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, MinMaxScaler, RobustScaler, Imputer\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "                    .appName(\"Group 13 CRWK\") \\\n",
        "                    .master(\"local[*]\") \\\n",
        "                    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "                    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "                    .config(\"spark.executor.cores\", \"2\") \\\n",
        "                    .config(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\") \\\n",
        "                    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHft1Jht1Qxl"
      },
      "source": [
        "---\n",
        "# **Task 1 - Data Loading and Preprocessing (15 marks)**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "OcZfiIcq1Qxn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------+-------------------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+---------------+-------------+----------------+----------------+------------+------------+-----------+----------------+----------------+-----------+-----------+-----------+----------------+----------------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+-------------+------------+-----------+-----------+--------------+--------------+----------------+------------+------------+------------+------------+------------+------------+--------------+------------+-------------+--------------+----------------+----------------+--------------+--------------+----------------+--------------+--------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+----------+--------------+--------+--------+------+\n",
            "|Dst Port|Protocol|Timestamp          |Flow Duration|Tot Fwd Pkts|Tot Bwd Pkts|TotLen Fwd Pkts|TotLen Bwd Pkts|Fwd Pkt Len Max|Fwd Pkt Len Min|Fwd Pkt Len Mean|Fwd Pkt Len Std|Bwd Pkt Len Max|Bwd Pkt Len Min|Bwd Pkt Len Mean|Bwd Pkt Len Std|Flow Byts/s    |Flow Pkts/s  |Flow IAT Mean   |Flow IAT Std    |Flow IAT Max|Flow IAT Min|Fwd IAT Tot|Fwd IAT Mean    |Fwd IAT Std     |Fwd IAT Max|Fwd IAT Min|Bwd IAT Tot|Bwd IAT Mean    |Bwd IAT Std     |Bwd IAT Max|Bwd IAT Min|Fwd PSH Flags|Bwd PSH Flags|Fwd URG Flags|Bwd URG Flags|Fwd Header Len|Bwd Header Len|Fwd Pkts/s   |Bwd Pkts/s  |Pkt Len Min|Pkt Len Max|Pkt Len Mean  |Pkt Len Std   |Pkt Len Var     |FIN Flag Cnt|SYN Flag Cnt|RST Flag Cnt|PSH Flag Cnt|ACK Flag Cnt|URG Flag Cnt|CWE Flag Count|ECE Flag Cnt|Down/Up Ratio|Pkt Size Avg  |Fwd Seg Size Avg|Bwd Seg Size Avg|Fwd Byts/b Avg|Fwd Pkts/b Avg|Fwd Blk Rate Avg|Bwd Byts/b Avg|Bwd Pkts/b Avg|Bwd Blk Rate Avg|Subflow Fwd Pkts|Subflow Fwd Byts|Subflow Bwd Pkts|Subflow Bwd Byts|Init Fwd Win Byts|Init Bwd Win Byts|Fwd Act Data Pkts|Fwd Seg Size Min|Active Mean|Active Std|Active Max|Active Min|Idle Mean |Idle Std      |Idle Max|Idle Min|Label |\n",
            "+--------+--------+-------------------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+---------------+-------------+----------------+----------------+------------+------------+-----------+----------------+----------------+-----------+-----------+-----------+----------------+----------------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+-------------+------------+-----------+-----------+--------------+--------------+----------------+------------+------------+------------+------------+------------+------------+--------------+------------+-------------+--------------+----------------+----------------+--------------+--------------+----------------+--------------+--------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+----------+--------------+--------+--------+------+\n",
            "|0       |0       |14/02/2018 08:31:01|112641719    |3           |0           |0              |0              |0              |0              |0               |0              |0              |0              |0               |0              |0              |0.0266331163 |56320859.5      |139.3000358938  |56320958    |56320761    |112641719  |56320859.5      |139.3000358938  |56320958   |56320761   |0          |0               |0               |0          |0          |0            |0            |0            |0            |0             |0             |0.0266331163 |0           |0          |0          |0             |0             |0               |0           |0           |0           |0           |0           |0           |0             |0           |0            |0             |0               |0               |0             |0             |0               |0             |0             |0               |3               |0               |0               |0               |-1               |-1               |0                |0               |0          |0         |0         |0         |56320859.5|139.3000358938|56320958|56320761|Benign|\n",
            "|0       |0       |14/02/2018 08:33:50|112641466    |3           |0           |0              |0              |0              |0              |0               |0              |0              |0              |0               |0              |0              |0.0266331761 |56320733        |114.5512985522  |56320814    |56320652    |112641466  |56320733        |114.5512985522  |56320814   |56320652   |0          |0               |0               |0          |0          |0            |0            |0            |0            |0             |0             |0.0266331761 |0           |0          |0          |0             |0             |0               |0           |0           |0           |0           |0           |0           |0             |0           |0            |0             |0               |0               |0             |0             |0               |0             |0             |0               |3               |0               |0               |0               |-1               |-1               |0                |0               |0          |0         |0         |0         |56320733  |114.5512985522|56320814|56320652|Benign|\n",
            "|0       |0       |14/02/2018 08:36:39|112638623    |3           |0           |0              |0              |0              |0              |0               |0              |0              |0              |0               |0              |0              |0.0266338483 |56319311.5      |301.9345955667  |56319525    |56319098    |112638623  |56319311.5      |301.9345955667  |56319525   |56319098   |0          |0               |0               |0          |0          |0            |0            |0            |0            |0             |0             |0.0266338483 |0           |0          |0          |0             |0             |0               |0           |0           |0           |0           |0           |0           |0             |0           |0            |0             |0               |0               |0             |0             |0               |0             |0             |0               |3               |0               |0               |0               |-1               |-1               |0                |0               |0          |0         |0         |0         |56319311.5|301.9345955667|56319525|56319098|Benign|\n",
            "|22      |6       |14/02/2018 08:40:13|6453966      |15          |10          |1239           |2273           |744            |0              |82.6            |196.7412368715 |976            |0              |227.3           |371.6778922072 |544.1615279659 |3.8735871865 |268915.25       |247443.778966007|673900      |22          |6453966    |460997.571428571|123109.423587757|673900     |229740     |5637902    |626433.555555556|455082.21422401 |1167293    |554        |0            |0            |0            |0            |488           |328           |2.3241523119 |1.5494348746|0          |976        |135.0769230769|277.8347599674|77192.1538461539|0           |0           |0           |1           |0           |0           |0             |0           |0            |140.48        |82.6            |227.3           |0             |0             |0               |0             |0             |0               |15              |1239            |10              |2273            |65535            |233              |6                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|22      |6       |14/02/2018 08:40:23|8804066      |14          |11          |1143           |2209           |744            |0              |81.6428571429   |203.7455453568 |976            |0              |200.8181818182  |362.2498635422 |380.7331748762 |2.839597068  |366836.083333333|511356.609732762|1928102     |21          |8804066    |677235.846153846|532416.970958985|1928102    |246924     |7715481    |771548.1        |755543.082716951|2174893    |90         |0            |0            |0            |0            |456           |360           |1.5901743581 |1.2494227099|0          |976        |128.9230769231|279.7630315931|78267.3538461539|0           |0           |0           |1           |0           |0           |0             |0           |0            |134.08        |81.6428571429   |200.8181818182  |0             |0             |0               |0             |0             |0               |14              |1143            |11              |2209            |5808             |233              |6                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|22      |6       |14/02/2018 08:40:31|6989341      |16          |12          |1239           |2273           |744            |0              |77.4375         |190.8311535538 |976            |0              |189.4166666667  |347.6425694023 |502.4794183028 |4.0061001459 |258864.481481481|291724.14791076 |951098      |20          |6989341    |465956.066666667|244363.896416351|951098     |265831     |5980598    |543690.727272727|460713.519752371|1254338    |78         |0            |0            |0            |0            |332           |252           |2.2892000834 |1.7169000625|0          |976        |121.1034482759|265.7086676402|70601.0960591133|0           |0           |0           |1           |0           |0           |0             |0           |0            |125.4285714286|77.4375         |189.4166666667  |0             |0             |0               |0             |0             |0               |16              |1239            |12              |2273            |5808             |234              |7                |20              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|0       |0       |14/02/2018 08:39:28|112640480    |3           |0           |0              |0              |0              |0              |0               |0              |0              |0              |0               |0              |0              |0.0266334092 |56320240        |203.6467529817  |56320384    |56320096    |112640480  |56320240        |203.6467529817  |56320384   |56320096   |0          |0               |0               |0          |0          |0            |0            |0            |0            |0             |0             |0.0266334092 |0           |0          |0          |0             |0             |0               |0           |0           |0           |0           |0           |0           |0             |0           |0            |0             |0               |0               |0             |0             |0               |0             |0             |0               |3               |0               |0               |0               |-1               |-1               |0                |0               |0          |0         |0         |0         |56320240  |203.6467529817|56320384|56320096|Benign|\n",
            "|0       |0       |14/02/2018 08:42:17|112641244    |3           |0           |0              |0              |0              |0              |0               |0              |0              |0              |0               |0              |0              |0.0266332286 |56320622        |62.2253967444   |56320666    |56320578    |112641244  |56320622        |62.2253967444   |56320666   |56320578   |0          |0               |0               |0          |0          |0            |0            |0            |0            |0             |0             |0.0266332286 |0           |0          |0          |0             |0             |0               |0           |0           |0           |0           |0           |0           |0             |0           |0            |0             |0               |0               |0             |0             |0               |0             |0             |0               |3               |0               |0               |0               |-1               |-1               |0                |0               |0          |0         |0         |0         |56320622  |62.2253967444 |56320666|56320578|Benign|\n",
            "|80      |6       |14/02/2018 08:47:14|476513       |5           |3           |211            |463            |211            |0              |42.2            |94.3620686505  |463            |0              |154.3333333333  |267.3131746348 |1414.4419984345|16.7886290615|68073.2857142857|115865.792656438|237711      |24          |476513     |119128.25       |137379.963358017|238470     |108        |238634     |119317          |167621.076693833|237843     |791        |0            |0            |0            |0            |168           |104           |10.4928931635|6.2957358981|0          |463        |74.8888888889 |161.4058893322|26051.8611111111|0           |0           |0           |1           |0           |0           |0             |0           |0            |84.25         |42.2            |154.3333333333  |0             |0             |0               |0             |0             |0               |5               |211             |3               |463             |14600            |219              |1                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|80      |6       |14/02/2018 08:47:15|475048       |5           |3           |220            |472            |220            |0              |44              |98.38699101    |472            |0              |157.3333333333  |272.5093270575 |1456.6949024099|16.8404034961|67864           |115746.933154476|237494      |15          |475048     |118762          |137096.759626185|237853     |15         |237516     |118758          |167472.584269784|237179     |337        |0            |0            |0            |0            |168           |104           |10.525252185 |6.315151311 |0          |472        |76.8888888889 |165.0669897681|27247.1111111111|0           |0           |0           |1           |0           |0           |0             |0           |0            |86.5          |44              |157.3333333333  |0             |0             |0               |0             |0             |0               |5               |220             |3               |472             |14600            |219              |1                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|80      |6       |14/02/2018 08:47:15|474926       |5           |3           |220            |472            |220            |0              |44              |98.38699101    |472            |0              |157.3333333333  |272.5093270575 |1457.0691012916|16.8447294947|67846.5714285714|115645.740842248|237162      |15          |474926     |118731.5        |136923.365842601|237497     |15         |237732     |118866          |167663.503100705|237422     |310        |0            |0            |0            |0            |168           |104           |10.5279559342|6.3167735605|0          |472        |76.8888888889 |165.0669897681|27247.1111111111|0           |0           |0           |1           |0           |0           |0             |0           |0            |86.5          |44              |157.3333333333  |0             |0             |0               |0             |0             |0               |5               |220             |3               |472             |14480            |219              |1                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|80      |6       |14/02/2018 08:47:16|477471       |5           |3           |209            |461            |209            |0              |41.8            |93.4676414595  |461            |0              |153.6666666667  |266.1584740964 |1403.2265833946|16.7549442793|68210.1428571429|116178.228792989|238389      |17          |477471     |119367.75       |137516.508224467|238515     |149        |238887     |119443.5        |168454.755588852|238559     |328        |0            |0            |0            |0            |168           |104           |10.4718401746|6.2831041048|0          |461        |74.4444444444 |160.5942955954|25790.5277777778|0           |0           |0           |1           |0           |0           |0             |0           |0            |83.75         |41.8            |153.6666666667  |0             |0             |0               |0             |0             |0               |5               |209             |3               |461             |14480            |219              |1                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|80      |6       |14/02/2018 08:47:16|512758       |5           |3           |211            |463            |211            |0              |42.2            |94.3620686505  |463            |0              |154.3333333333  |267.3131746348 |1314.4602327024|15.6019018718|73251.1428571429|124959.473740394|256188      |10          |512758     |128189.5        |148006.106082373|256523     |10         |256563     |128281.5        |180935.190276795|256222     |341        |0            |0            |0            |0            |168           |104           |9.7511886699 |5.8507132019|0          |463        |74.8888888889 |161.4058893322|26051.8611111111|0           |0           |0           |1           |0           |0           |0             |0           |0            |84.25         |42.2            |154.3333333333  |0             |0             |0               |0             |0             |0               |5               |211             |3               |463             |14480            |219              |1                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|80      |6       |14/02/2018 08:47:17|476711       |5           |3           |206            |458            |206            |0              |41.2            |92.126000673   |458            |0              |152.6666666667  |264.4264232888 |1392.8774456641|16.7816559719|68101.5714285714|115977.215079597|238034      |16          |476711     |119177.75       |137207.36560009 |238274     |78         |238033     |119016.5        |168007.864103143|237816     |217        |0            |0            |0            |0            |168           |104           |10.4885349824|6.2931209894|0          |458        |73.7777777778 |159.3783060659|25401.4444444444|0           |0           |0           |1           |0           |0           |0             |0           |0            |83            |41.2            |152.6666666667  |0             |0             |0               |0             |0             |0               |5               |206             |3               |458             |14480            |219              |1                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|80      |6       |14/02/2018 08:47:17|476616       |5           |3           |211            |463            |211            |0              |42.2            |94.3620686505  |463            |0              |154.3333333333  |267.3131746348 |1414.1363277775|16.7850009232|68088           |115553.073301117|237285      |8           |476616     |119154          |136576.972942001|237558     |8          |237660     |118830          |167563.093937776|237315     |345        |0            |0            |0            |0            |168           |104           |10.490625577 |6.2943753462|0          |463        |74.8888888889 |161.4058893322|26051.8611111111|0           |0           |0           |1           |0           |0           |0             |0           |0            |84.25         |42.2            |154.3333333333  |0             |0             |0               |0             |0             |0               |5               |211             |3               |463             |14480            |219              |1                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|80      |6       |14/02/2018 08:47:18|477161       |5           |3           |211            |463            |211            |0              |42.2            |94.3620686505  |463            |0              |154.3333333333  |267.3131746348 |1412.5211406632|16.7658295628|68165.8571428572|116324.530541969|238504      |12          |477161     |119290.25       |137729.560342905|238719     |12         |238618     |119309          |168449.805841384|238421     |197        |0            |0            |0            |0            |168           |104           |10.4786434767|6.287186086 |0          |463        |74.8888888889 |161.4058893322|26051.8611111111|0           |0           |0           |1           |0           |0           |0             |0           |0            |84.25         |42.2            |154.3333333333  |0             |0             |0               |0             |0             |0               |5               |211             |3               |463             |14480            |219              |1                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|80      |6       |14/02/2018 08:47:18|474670       |5           |3           |214            |466            |214            |0              |42.8            |95.703709437   |466            |0              |155.3333333333  |269.0452254424 |1432.5742094508|16.8538142288|67810           |115371.262364883|236717      |15          |474670     |118667.5        |136423.221362787|236894     |55         |236992     |118496          |167297.22178805 |236793     |199        |0            |0            |0            |0            |168           |104           |10.533633893 |6.3201803358|0          |466        |75.5555555556 |162.6246530443|26446.7777777778|0           |0           |0           |1           |0           |0           |0             |0           |0            |85            |42.8            |155.3333333333  |0             |0             |0               |0             |0             |0               |5               |214             |3               |466             |14480            |219              |1                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|80      |6       |14/02/2018 08:47:19|476608       |5           |3           |209            |461            |209            |0              |41.8            |93.4676414595  |461            |0              |153.6666666667  |266.1584740964 |1405.7674231234|16.7852826642|68086.8571428572|116165.357657993|238154      |21          |476608     |119152          |137536.399945614|238349     |28         |238442     |119221          |168305.556058022|238231     |211        |0            |0            |0            |0            |168           |104           |10.4908016651|6.2944809991|0          |461        |74.4444444444 |160.5942955954|25790.5277777778|0           |0           |0           |1           |0           |0           |0             |0           |0            |83.75         |41.8            |153.6666666667  |0             |0             |0               |0             |0             |0               |5               |209             |3               |461             |14480            |219              |1                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|80      |6       |14/02/2018 08:47:19|479249       |5           |3           |215            |467            |215            |0              |43              |96.1509230325  |467            |0              |155.6666666667  |269.6225757116 |1423.0598290242|16.6927839182|68464.1428571429|116522.185511928|239061      |18          |479249     |119812.25       |137797.592674606|239270     |21         |239238     |119619          |168903.768394906|239052     |186        |0            |0            |0            |0            |168           |104           |10.4329899489|6.2597939693|0          |467        |75.7777777778 |163.0312683029|26579.1944444444|0           |0           |0           |1           |0           |0           |0             |0           |0            |85.25         |43              |155.6666666667  |0             |0             |0               |0             |0             |0               |5               |215             |3               |467             |14480            |219              |1                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "|80      |6       |14/02/2018 08:47:20|475967       |5           |3           |215            |467            |215            |0              |43              |96.1509230325  |467            |0              |155.6666666667  |269.6225757116 |1432.8724470394|16.8078879418|67995.2857142857|115929.081086548|237703      |16          |475967     |118991.75       |137195.538016305|237904     |21         |237915     |118957.5        |167975.337191208|237734     |181        |0            |0            |0            |0            |168           |104           |10.5049299636|6.3029579782|0          |467        |75.7777777778 |163.0312683029|26579.1944444444|0           |0           |0           |1           |0           |0           |0             |0           |0            |85.25         |43              |155.6666666667  |0             |0             |0               |0             |0             |0               |5               |215             |3               |467             |14480            |219              |1                |32              |0          |0         |0         |0         |0         |0             |0       |0       |Benign|\n",
            "+--------+--------+-------------------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+---------------+-------------+----------------+----------------+------------+------------+-----------+----------------+----------------+-----------+-----------+-----------+----------------+----------------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+-------------+------------+-----------+-----------+--------------+--------------+----------------+------------+------------+------------+------------+------------+------------+--------------+------------+-------------+--------------+----------------+----------------+--------------+--------------+----------------+--------------+--------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+----------+--------------+--------+--------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 6305:============================>                           (2 + 2) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1048575 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#Identify the student who made a contribution and mention their name in the appropriate section of the code.\n",
        "\n",
        "## Philip Acquaye-Mensah\n",
        "\n",
        "# Load the compressed file as a text file\n",
        "df = spark.read.csv(\"CourseWork_Dataset_Machine_Learning.csv\", header = True)\n",
        "# Display the DataFrame\n",
        "df.show(20, truncate= False)\n",
        "\n",
        "# more info\n",
        "print(df.count(), df.rdd.getNumPartitions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "# Exclude 'Label' column from casting\n",
        "columns_to_cast = [column for column in df.columns if column != 'Label']\n",
        "\n",
        "for column in columns_to_cast:\n",
        "    new_column = column.replace(' ', '_').replace('/', '_per_')\n",
        "    df = df.withColumnRenamed(column, new_column).withColumn(new_column, col(new_column).cast(FloatType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+\n",
            "|         Label|\n",
            "+--------------+\n",
            "|SSH-Bruteforce|\n",
            "|        Benign|\n",
            "|FTP-BruteForce|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check Multi class label\n",
        "# Label is the column named Label\n",
        "df.select(\"Label\").distinct().show(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 6311:============================>                           (2 + 2) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+------------+\n",
            "|         Label|Label_binary|\n",
            "+--------------+------------+\n",
            "|FTP-BruteForce|           1|\n",
            "|SSH-Bruteforce|           1|\n",
            "|        Benign|           0|\n",
            "+--------------+------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# Convert \"Label\" into a binary classification where \"Benign\" is 0 and any attack is 1\n",
        "df = df.withColumn(\"Label_binary\", when(col(\"Label\") == \"Benign\", 0).otherwise(1))\n",
        "\n",
        "# Display the updated DataFrame to verify the transformation\n",
        "df.select(\"Label\", \"Label_binary\").distinct().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Dst_Port: float (nullable = true)\n",
            " |-- Protocol: float (nullable = true)\n",
            " |-- Flow_Duration: float (nullable = true)\n",
            " |-- Tot_Fwd_Pkts: float (nullable = true)\n",
            " |-- Tot_Bwd_Pkts: float (nullable = true)\n",
            " |-- TotLen_Fwd_Pkts: float (nullable = true)\n",
            " |-- TotLen_Bwd_Pkts: float (nullable = true)\n",
            " |-- Fwd_Pkt_Len_Max: float (nullable = true)\n",
            " |-- Fwd_Pkt_Len_Min: float (nullable = true)\n",
            " |-- Fwd_Pkt_Len_Mean: float (nullable = true)\n",
            " |-- Fwd_Pkt_Len_Std: float (nullable = true)\n",
            " |-- Bwd_Pkt_Len_Max: float (nullable = true)\n",
            " |-- Bwd_Pkt_Len_Min: float (nullable = true)\n",
            " |-- Bwd_Pkt_Len_Mean: float (nullable = true)\n",
            " |-- Bwd_Pkt_Len_Std: float (nullable = true)\n",
            " |-- Flow_Byts_per_s: float (nullable = true)\n",
            " |-- Flow_Pkts_per_s: float (nullable = true)\n",
            " |-- Flow_IAT_Mean: float (nullable = true)\n",
            " |-- Flow_IAT_Std: float (nullable = true)\n",
            " |-- Flow_IAT_Max: float (nullable = true)\n",
            " |-- Flow_IAT_Min: float (nullable = true)\n",
            " |-- Fwd_IAT_Tot: float (nullable = true)\n",
            " |-- Fwd_IAT_Mean: float (nullable = true)\n",
            " |-- Fwd_IAT_Std: float (nullable = true)\n",
            " |-- Fwd_IAT_Max: float (nullable = true)\n",
            " |-- Fwd_IAT_Min: float (nullable = true)\n",
            " |-- Bwd_IAT_Tot: float (nullable = true)\n",
            " |-- Bwd_IAT_Mean: float (nullable = true)\n",
            " |-- Bwd_IAT_Std: float (nullable = true)\n",
            " |-- Bwd_IAT_Max: float (nullable = true)\n",
            " |-- Bwd_IAT_Min: float (nullable = true)\n",
            " |-- Fwd_PSH_Flags: float (nullable = true)\n",
            " |-- Bwd_PSH_Flags: float (nullable = true)\n",
            " |-- Fwd_URG_Flags: float (nullable = true)\n",
            " |-- Bwd_URG_Flags: float (nullable = true)\n",
            " |-- Fwd_Header_Len: float (nullable = true)\n",
            " |-- Bwd_Header_Len: float (nullable = true)\n",
            " |-- Fwd_Pkts_per_s: float (nullable = true)\n",
            " |-- Bwd_Pkts_per_s: float (nullable = true)\n",
            " |-- Pkt_Len_Min: float (nullable = true)\n",
            " |-- Pkt_Len_Max: float (nullable = true)\n",
            " |-- Pkt_Len_Mean: float (nullable = true)\n",
            " |-- Pkt_Len_Std: float (nullable = true)\n",
            " |-- Pkt_Len_Var: float (nullable = true)\n",
            " |-- FIN_Flag_Cnt: float (nullable = true)\n",
            " |-- SYN_Flag_Cnt: float (nullable = true)\n",
            " |-- RST_Flag_Cnt: float (nullable = true)\n",
            " |-- PSH_Flag_Cnt: float (nullable = true)\n",
            " |-- ACK_Flag_Cnt: float (nullable = true)\n",
            " |-- URG_Flag_Cnt: float (nullable = true)\n",
            " |-- CWE_Flag_Count: float (nullable = true)\n",
            " |-- ECE_Flag_Cnt: float (nullable = true)\n",
            " |-- Down_per_Up_Ratio: float (nullable = true)\n",
            " |-- Pkt_Size_Avg: float (nullable = true)\n",
            " |-- Fwd_Seg_Size_Avg: float (nullable = true)\n",
            " |-- Bwd_Seg_Size_Avg: float (nullable = true)\n",
            " |-- Fwd_Byts_per_b_Avg: float (nullable = true)\n",
            " |-- Fwd_Pkts_per_b_Avg: float (nullable = true)\n",
            " |-- Fwd_Blk_Rate_Avg: float (nullable = true)\n",
            " |-- Bwd_Byts_per_b_Avg: float (nullable = true)\n",
            " |-- Bwd_Pkts_per_b_Avg: float (nullable = true)\n",
            " |-- Bwd_Blk_Rate_Avg: float (nullable = true)\n",
            " |-- Subflow_Fwd_Pkts: float (nullable = true)\n",
            " |-- Subflow_Fwd_Byts: float (nullable = true)\n",
            " |-- Subflow_Bwd_Pkts: float (nullable = true)\n",
            " |-- Subflow_Bwd_Byts: float (nullable = true)\n",
            " |-- Init_Fwd_Win_Byts: float (nullable = true)\n",
            " |-- Init_Bwd_Win_Byts: float (nullable = true)\n",
            " |-- Fwd_Act_Data_Pkts: float (nullable = true)\n",
            " |-- Fwd_Seg_Size_Min: float (nullable = true)\n",
            " |-- Active_Mean: float (nullable = true)\n",
            " |-- Active_Std: float (nullable = true)\n",
            " |-- Active_Max: float (nullable = true)\n",
            " |-- Active_Min: float (nullable = true)\n",
            " |-- Idle_Mean: float (nullable = true)\n",
            " |-- Idle_Std: float (nullable = true)\n",
            " |-- Idle_Max: float (nullable = true)\n",
            " |-- Idle_Min: float (nullable = true)\n",
            " |-- Label_binary: integer (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Drop Label column because of new binary label and Timestamp column\n",
        "df = df.drop(\"Label\",\"Timestamp\")\n",
        "\n",
        "# Print the schema to see the data types and structure\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 6314:============================>                           (2 + 2) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------+\n",
            "|Label_binary| count|\n",
            "+------------+------+\n",
            "|           1|380949|\n",
            "|           0|667626|\n",
            "+------------+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Check for how many attacks and benign\n",
        "df.groupBy(\"Label_binary\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 6317:==========================================>             (3 + 1) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-----------------+------------+----------------+----------------+------------------+------------------+----------------+------------------+------------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+--------+--------+--------+------------+\n",
            "|Dst_Port|Protocol|Flow_Duration|Tot_Fwd_Pkts|Tot_Bwd_Pkts|TotLen_Fwd_Pkts|TotLen_Bwd_Pkts|Fwd_Pkt_Len_Max|Fwd_Pkt_Len_Min|Fwd_Pkt_Len_Mean|Fwd_Pkt_Len_Std|Bwd_Pkt_Len_Max|Bwd_Pkt_Len_Min|Bwd_Pkt_Len_Mean|Bwd_Pkt_Len_Std|Flow_Byts_per_s|Flow_Pkts_per_s|Flow_IAT_Mean|Flow_IAT_Std|Flow_IAT_Max|Flow_IAT_Min|Fwd_IAT_Tot|Fwd_IAT_Mean|Fwd_IAT_Std|Fwd_IAT_Max|Fwd_IAT_Min|Bwd_IAT_Tot|Bwd_IAT_Mean|Bwd_IAT_Std|Bwd_IAT_Max|Bwd_IAT_Min|Fwd_PSH_Flags|Bwd_PSH_Flags|Fwd_URG_Flags|Bwd_URG_Flags|Fwd_Header_Len|Bwd_Header_Len|Fwd_Pkts_per_s|Bwd_Pkts_per_s|Pkt_Len_Min|Pkt_Len_Max|Pkt_Len_Mean|Pkt_Len_Std|Pkt_Len_Var|FIN_Flag_Cnt|SYN_Flag_Cnt|RST_Flag_Cnt|PSH_Flag_Cnt|ACK_Flag_Cnt|URG_Flag_Cnt|CWE_Flag_Count|ECE_Flag_Cnt|Down_per_Up_Ratio|Pkt_Size_Avg|Fwd_Seg_Size_Avg|Bwd_Seg_Size_Avg|Fwd_Byts_per_b_Avg|Fwd_Pkts_per_b_Avg|Fwd_Blk_Rate_Avg|Bwd_Byts_per_b_Avg|Bwd_Pkts_per_b_Avg|Bwd_Blk_Rate_Avg|Subflow_Fwd_Pkts|Subflow_Fwd_Byts|Subflow_Bwd_Pkts|Subflow_Bwd_Byts|Init_Fwd_Win_Byts|Init_Bwd_Win_Byts|Fwd_Act_Data_Pkts|Fwd_Seg_Size_Min|Active_Mean|Active_Std|Active_Max|Active_Min|Idle_Mean|Idle_Std|Idle_Max|Idle_Min|Label_binary|\n",
            "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-----------------+------------+----------------+----------------+------------------+------------------+----------------+------------------+------------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+--------+--------+--------+------------+\n",
            "|       0|       0|            0|           0|           0|              0|              0|              0|              0|               0|              0|              0|              0|               0|              0|           2277|              0|            0|           0|           0|           0|          0|           0|          0|          0|          0|          0|           0|          0|          0|          0|            0|            0|            0|            0|             0|             0|             0|             0|          0|          0|           0|          0|          0|           0|           0|           0|           0|           0|           0|             0|           0|                0|           0|               0|               0|                 0|                 0|               0|                 0|                 0|               0|               0|               0|               0|               0|                0|                0|                0|               0|          0|         0|         0|         0|        0|       0|       0|       0|           0|\n",
            "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-----------------+------------+----------------+----------------+------------------+------------------+----------------+------------------+------------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+--------+--------+--------+------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Check for missing values in each column\n",
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Dst_Port: float (nullable = true)\n",
            " |-- Protocol: float (nullable = true)\n",
            " |-- Flow_Duration: float (nullable = true)\n",
            " |-- Tot_Fwd_Pkts: float (nullable = true)\n",
            " |-- Tot_Bwd_Pkts: float (nullable = true)\n",
            " |-- TotLen_Fwd_Pkts: float (nullable = true)\n",
            " |-- TotLen_Bwd_Pkts: float (nullable = true)\n",
            " |-- Fwd_Pkt_Len_Max: float (nullable = true)\n",
            " |-- Fwd_Pkt_Len_Min: float (nullable = true)\n",
            " |-- Fwd_Pkt_Len_Mean: float (nullable = true)\n",
            " |-- Fwd_Pkt_Len_Std: float (nullable = true)\n",
            " |-- Bwd_Pkt_Len_Max: float (nullable = true)\n",
            " |-- Bwd_Pkt_Len_Min: float (nullable = true)\n",
            " |-- Bwd_Pkt_Len_Mean: float (nullable = true)\n",
            " |-- Bwd_Pkt_Len_Std: float (nullable = true)\n",
            " |-- Flow_Pkts_per_s: float (nullable = true)\n",
            " |-- Flow_IAT_Mean: float (nullable = true)\n",
            " |-- Flow_IAT_Std: float (nullable = true)\n",
            " |-- Flow_IAT_Max: float (nullable = true)\n",
            " |-- Flow_IAT_Min: float (nullable = true)\n",
            " |-- Fwd_IAT_Tot: float (nullable = true)\n",
            " |-- Fwd_IAT_Mean: float (nullable = true)\n",
            " |-- Fwd_IAT_Std: float (nullable = true)\n",
            " |-- Fwd_IAT_Max: float (nullable = true)\n",
            " |-- Fwd_IAT_Min: float (nullable = true)\n",
            " |-- Bwd_IAT_Tot: float (nullable = true)\n",
            " |-- Bwd_IAT_Mean: float (nullable = true)\n",
            " |-- Bwd_IAT_Std: float (nullable = true)\n",
            " |-- Bwd_IAT_Max: float (nullable = true)\n",
            " |-- Bwd_IAT_Min: float (nullable = true)\n",
            " |-- Fwd_PSH_Flags: float (nullable = true)\n",
            " |-- Bwd_PSH_Flags: float (nullable = true)\n",
            " |-- Fwd_URG_Flags: float (nullable = true)\n",
            " |-- Bwd_URG_Flags: float (nullable = true)\n",
            " |-- Fwd_Header_Len: float (nullable = true)\n",
            " |-- Bwd_Header_Len: float (nullable = true)\n",
            " |-- Fwd_Pkts_per_s: float (nullable = true)\n",
            " |-- Bwd_Pkts_per_s: float (nullable = true)\n",
            " |-- Pkt_Len_Min: float (nullable = true)\n",
            " |-- Pkt_Len_Max: float (nullable = true)\n",
            " |-- Pkt_Len_Mean: float (nullable = true)\n",
            " |-- Pkt_Len_Std: float (nullable = true)\n",
            " |-- Pkt_Len_Var: float (nullable = true)\n",
            " |-- FIN_Flag_Cnt: float (nullable = true)\n",
            " |-- SYN_Flag_Cnt: float (nullable = true)\n",
            " |-- RST_Flag_Cnt: float (nullable = true)\n",
            " |-- PSH_Flag_Cnt: float (nullable = true)\n",
            " |-- ACK_Flag_Cnt: float (nullable = true)\n",
            " |-- URG_Flag_Cnt: float (nullable = true)\n",
            " |-- CWE_Flag_Count: float (nullable = true)\n",
            " |-- ECE_Flag_Cnt: float (nullable = true)\n",
            " |-- Down_per_Up_Ratio: float (nullable = true)\n",
            " |-- Pkt_Size_Avg: float (nullable = true)\n",
            " |-- Fwd_Seg_Size_Avg: float (nullable = true)\n",
            " |-- Bwd_Seg_Size_Avg: float (nullable = true)\n",
            " |-- Fwd_Byts_per_b_Avg: float (nullable = true)\n",
            " |-- Fwd_Pkts_per_b_Avg: float (nullable = true)\n",
            " |-- Fwd_Blk_Rate_Avg: float (nullable = true)\n",
            " |-- Bwd_Byts_per_b_Avg: float (nullable = true)\n",
            " |-- Bwd_Pkts_per_b_Avg: float (nullable = true)\n",
            " |-- Bwd_Blk_Rate_Avg: float (nullable = true)\n",
            " |-- Subflow_Fwd_Pkts: float (nullable = true)\n",
            " |-- Subflow_Fwd_Byts: float (nullable = true)\n",
            " |-- Subflow_Bwd_Pkts: float (nullable = true)\n",
            " |-- Subflow_Bwd_Byts: float (nullable = true)\n",
            " |-- Init_Fwd_Win_Byts: float (nullable = true)\n",
            " |-- Init_Bwd_Win_Byts: float (nullable = true)\n",
            " |-- Fwd_Act_Data_Pkts: float (nullable = true)\n",
            " |-- Fwd_Seg_Size_Min: float (nullable = true)\n",
            " |-- Active_Mean: float (nullable = true)\n",
            " |-- Active_Std: float (nullable = true)\n",
            " |-- Active_Max: float (nullable = true)\n",
            " |-- Active_Min: float (nullable = true)\n",
            " |-- Idle_Mean: float (nullable = true)\n",
            " |-- Idle_Std: float (nullable = true)\n",
            " |-- Idle_Max: float (nullable = true)\n",
            " |-- Idle_Min: float (nullable = true)\n",
            " |-- Label_binary: integer (nullable = false)\n",
            " |-- Flow_Byts_s_imputed: float (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "\n",
        "# Now, use the Imputer on the renamed column\n",
        "imputer = Imputer(\n",
        "    inputCols=[\"Flow_Byts_per_s\"],\n",
        "    outputCols=[\"Flow_Byts_s_imputed\"]\n",
        ").setStrategy(\"median\")\n",
        "\n",
        "df = imputer.fit(df).transform(df)\n",
        "\n",
        "# Drop the 'Flow_Byts_per_s' column\n",
        "df = df.drop('Flow_Byts_per_s')\n",
        "\n",
        "# Print Schema again to check the changes\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 6323:>                                                       (0 + 4) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---------------+\n",
            "|summary|Flow_Pkts_per_s|\n",
            "+-------+---------------+\n",
            "|  count|        1048575|\n",
            "|   mean|       Infinity|\n",
            "| stddev|            NaN|\n",
            "|    min|  -0.0010297608|\n",
            "|    max|       Infinity|\n",
            "+-------+---------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df.describe(\"Flow_Pkts_per_s\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Flow_Duration' has 5 rows with negative values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Flow_Pkts_per_s' has 5 rows with negative values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Flow_Pkts_per_s' has 3824 rows with infinity values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Flow_IAT_Mean' has 5 rows with negative values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Flow_IAT_Max' has 2 rows with negative values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Flow_IAT_Min' has 5 rows with negative values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Fwd_IAT_Tot' has 5 rows with negative values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Fwd_IAT_Mean' has 5 rows with negative values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Fwd_IAT_Max' has 2 rows with negative values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Fwd_IAT_Min' has 5 rows with negative values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Init_Fwd_Win_Byts' has 219266 rows with negative values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Init_Bwd_Win_Byts' has 347370 rows with negative values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 7268:>                                                       (0 + 4) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Column 'Flow_Byts_s_imputed' has 1547 rows with infinity values.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, isnan\n",
        " \n",
        "# Iterate through each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Filter and count rows with null values\n",
        "    null_count = df.filter(col(column).isNull()).count()\n",
        "    if null_count > 0:\n",
        "        print(f\"Column '{column}' has {null_count} rows with null values.\")\n",
        "    # Filter and count rows with NaN values\n",
        "    nan_count = df.filter(isnan(col(column))).count()\n",
        "    if nan_count > 0:\n",
        "        print(f\"Column '{column}' has {nan_count} rows with NaN values.\")\n",
        "    # Filter and count rows with negative values\n",
        "    negative_count = df.filter(col(column) < 0).count()\n",
        "    if negative_count > 0:\n",
        "        print(f\"Column '{column}' has {negative_count} rows with negative values.\")\n",
        "        \n",
        "    # Filter and count rows with infinity values\n",
        "    infinity_count = df.filter(col(column) == float('inf')).count()\n",
        "    if infinity_count > 0:\n",
        "        print(f\"Column '{column}' has {infinity_count} rows with infinity values.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 7274:============================>                           (2 + 2) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------------------+\n",
            "|summary|Flow_Byts_s_nan_imputed|\n",
            "+-------+-----------------------+\n",
            "|  count|                1048575|\n",
            "|   mean|     193096.86274081515|\n",
            "| stddev|     3096074.8659511684|\n",
            "|    min|                    0.0|\n",
            "|    max|           4.45249984E8|\n",
            "+-------+-----------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# List of columns to replace Infinity values with NaN\n",
        "columns = [\"Flow_Byts_s_imputed\", \"Flow_Pkts_per_s\"]\n",
        "\n",
        "#! Loop through each column and replace Infinity with NaN\n",
        "for col_name in columns:\n",
        "    df = df.withColumn(col_name, when(col(col_name) == float('inf'), float('nan')).otherwise(col(col_name)))\n",
        "    \n",
        "#! Replace negative values with zero for all columns in the DataFrame\n",
        "for col_name in df.columns:\n",
        "    df = df.withColumn(col_name, when(df[col_name] < 0, 0).otherwise(df[col_name]))\n",
        "\n",
        "\n",
        "output_cols = [\"Flow_Byts_s_nan_imputed\", \"Flow_Pkts_per_s_nan_imputed\"]\n",
        "\n",
        "\n",
        "#! impute nan values with mean\n",
        "imputer = Imputer(\n",
        "    inputCols=columns,\n",
        "    outputCols=output_cols\n",
        ").setStrategy(\"median\")\n",
        "\n",
        "\n",
        "\n",
        "df = imputer.fit(df).transform(df)\n",
        "\n",
        "df.describe(\"Flow_Byts_s_nan_imputed\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-----------------+------------+----------------+----------------+------------------+------------------+----------------+------------------+------------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+-----------+---------+-----------+-----------+------------+-----------------------+---------------------------+\n",
            "|Dst_Port|Protocol|Flow_Duration|Tot_Fwd_Pkts|Tot_Bwd_Pkts|TotLen_Fwd_Pkts|TotLen_Bwd_Pkts|Fwd_Pkt_Len_Max|Fwd_Pkt_Len_Min|Fwd_Pkt_Len_Mean|Fwd_Pkt_Len_Std|Bwd_Pkt_Len_Max|Bwd_Pkt_Len_Min|Bwd_Pkt_Len_Mean|Bwd_Pkt_Len_Std|Flow_IAT_Mean|Flow_IAT_Std|Flow_IAT_Max|Flow_IAT_Min|Fwd_IAT_Tot |Fwd_IAT_Mean|Fwd_IAT_Std|Fwd_IAT_Max|Fwd_IAT_Min|Bwd_IAT_Tot|Bwd_IAT_Mean|Bwd_IAT_Std|Bwd_IAT_Max|Bwd_IAT_Min|Fwd_PSH_Flags|Bwd_PSH_Flags|Fwd_URG_Flags|Bwd_URG_Flags|Fwd_Header_Len|Bwd_Header_Len|Fwd_Pkts_per_s|Bwd_Pkts_per_s|Pkt_Len_Min|Pkt_Len_Max|Pkt_Len_Mean|Pkt_Len_Std|Pkt_Len_Var|FIN_Flag_Cnt|SYN_Flag_Cnt|RST_Flag_Cnt|PSH_Flag_Cnt|ACK_Flag_Cnt|URG_Flag_Cnt|CWE_Flag_Count|ECE_Flag_Cnt|Down_per_Up_Ratio|Pkt_Size_Avg|Fwd_Seg_Size_Avg|Bwd_Seg_Size_Avg|Fwd_Byts_per_b_Avg|Fwd_Pkts_per_b_Avg|Fwd_Blk_Rate_Avg|Bwd_Byts_per_b_Avg|Bwd_Pkts_per_b_Avg|Bwd_Blk_Rate_Avg|Subflow_Fwd_Pkts|Subflow_Fwd_Byts|Subflow_Bwd_Pkts|Subflow_Bwd_Byts|Init_Fwd_Win_Byts|Init_Bwd_Win_Byts|Fwd_Act_Data_Pkts|Fwd_Seg_Size_Min|Active_Mean|Active_Std|Active_Max|Active_Min|Idle_Mean  |Idle_Std |Idle_Max   |Idle_Min   |Label_binary|Flow_Byts_s_nan_imputed|Flow_Pkts_per_s_nan_imputed|\n",
            "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-----------------+------------+----------------+----------------+------------------+------------------+----------------+------------------+------------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+-----------+---------+-----------+-----------+------------+-----------------------+---------------------------+\n",
            "|0.0     |0.0     |1.1264172E8  |3.0         |0.0         |0.0            |0.0            |0.0            |0.0            |0.0             |0.0            |0.0            |0.0            |0.0             |0.0            |5.632086E7   |139.30003   |5.632096E7  |5.632076E7  |1.1264172E8 |5.632086E7  |139.30003  |5.632096E7 |5.632076E7 |0.0        |0.0         |0.0        |0.0        |0.0        |0.0          |0.0          |0.0          |0.0          |0.0           |0.0           |0.026633115   |0.0           |0.0        |0.0        |0.0         |0.0        |0.0        |0.0         |0.0         |0.0         |0.0         |0.0         |0.0         |0.0           |0.0         |0.0              |0.0         |0.0             |0.0             |0.0               |0.0               |0.0             |0.0               |0.0               |0.0             |3.0             |0.0             |0.0             |0.0             |0.0              |0.0              |0.0              |0.0             |0.0        |0.0       |0.0       |0.0       |5.632086E7 |139.30003|5.632096E7 |5.632076E7 |0           |0.0                    |0.026633115485310555       |\n",
            "|0.0     |0.0     |1.12641464E8 |3.0         |0.0         |0.0            |0.0            |0.0            |0.0            |0.0             |0.0            |0.0            |0.0            |0.0             |0.0            |5.6320732E7  |114.5513    |5.6320816E7 |5.6320652E7 |1.12641464E8|5.6320732E7 |114.5513   |5.6320816E7|5.6320652E7|0.0        |0.0         |0.0        |0.0        |0.0        |0.0          |0.0          |0.0          |0.0          |0.0           |0.0           |0.026633177   |0.0           |0.0        |0.0        |0.0         |0.0        |0.0        |0.0         |0.0         |0.0         |0.0         |0.0         |0.0         |0.0           |0.0         |0.0              |0.0         |0.0             |0.0             |0.0               |0.0               |0.0             |0.0               |0.0               |0.0             |3.0             |0.0             |0.0             |0.0             |0.0              |0.0              |0.0              |0.0             |0.0        |0.0       |0.0       |0.0       |5.6320732E7|114.5513 |5.6320816E7|5.6320652E7|0           |0.0                    |0.02663317695260048        |\n",
            "|0.0     |0.0     |1.12638624E8 |3.0         |0.0         |0.0            |0.0            |0.0            |0.0            |0.0             |0.0            |0.0            |0.0            |0.0             |0.0            |5.6319312E7  |301.9346    |5.6319524E7 |5.6319096E7 |1.12638624E8|5.6319312E7 |301.9346   |5.6319524E7|5.6319096E7|0.0        |0.0         |0.0        |0.0        |0.0        |0.0          |0.0          |0.0          |0.0          |0.0           |0.0           |0.026633848   |0.0           |0.0        |0.0        |0.0         |0.0        |0.0        |0.0         |0.0         |0.0         |0.0         |0.0         |0.0         |0.0           |0.0         |0.0              |0.0         |0.0             |0.0             |0.0               |0.0               |0.0             |0.0               |0.0               |0.0             |3.0             |0.0             |0.0             |0.0             |0.0              |0.0              |0.0              |0.0             |0.0        |0.0       |0.0       |0.0       |5.6319312E7|301.9346 |5.6319524E7|5.6319096E7|0           |0.0                    |0.026633847504854202       |\n",
            "|22.0    |6.0     |6453966.0    |15.0        |10.0        |1239.0         |2273.0         |744.0          |0.0            |82.6            |196.74124      |976.0          |0.0            |227.3           |371.6779       |268915.25    |247443.78   |673900.0    |22.0        |6453966.0   |460997.56   |123109.42  |673900.0   |229740.0   |5637902.0  |626433.56   |455082.22  |1167293.0  |554.0      |0.0          |0.0          |0.0          |0.0          |488.0         |328.0         |2.3241522     |1.5494349     |0.0        |976.0      |135.07692   |277.83475  |77192.16   |0.0         |0.0         |0.0         |1.0         |0.0         |0.0         |0.0           |0.0         |0.0              |140.48      |82.6            |227.3           |0.0               |0.0               |0.0             |0.0               |0.0               |0.0             |15.0            |1239.0          |10.0            |2273.0          |65535.0          |233.0            |6.0              |32.0            |0.0        |0.0       |0.0       |0.0       |0.0        |0.0      |0.0        |0.0        |0           |544.1614990234375      |3.873587131500244          |\n",
            "|22.0    |6.0     |8804066.0    |14.0        |11.0        |1143.0         |2209.0         |744.0          |0.0            |81.64286        |203.74554      |976.0          |0.0            |200.81818       |362.24988      |366836.1     |511356.62   |1928102.0   |21.0        |8804066.0   |677235.9    |532417.0   |1928102.0  |246924.0   |7715481.0  |771548.1    |755543.06  |2174893.0  |90.0       |0.0          |0.0          |0.0          |0.0          |456.0         |360.0         |1.5901743     |1.2494227     |0.0        |976.0      |128.92308   |279.76303  |78267.35   |0.0         |0.0         |0.0         |1.0         |0.0         |0.0         |0.0           |0.0         |0.0              |134.08      |81.64286        |200.81818       |0.0               |0.0               |0.0             |0.0               |0.0               |0.0             |14.0            |1143.0          |11.0            |2209.0          |5808.0           |233.0            |6.0              |32.0            |0.0        |0.0       |0.0       |0.0       |0.0        |0.0      |0.0        |0.0        |0           |380.7331848144531      |2.83959698677063           |\n",
            "|22.0    |6.0     |6989341.0    |16.0        |12.0        |1239.0         |2273.0         |744.0          |0.0            |77.4375         |190.83115      |976.0          |0.0            |189.41667       |347.64258      |258864.48    |291724.16   |951098.0    |20.0        |6989341.0   |465956.06   |244363.89  |951098.0   |265831.0   |5980598.0  |543690.75   |460713.53  |1254338.0  |78.0       |0.0          |0.0          |0.0          |0.0          |332.0         |252.0         |2.2892        |1.7169001     |0.0        |976.0      |121.10345   |265.70868  |70601.09   |0.0         |0.0         |0.0         |1.0         |0.0         |0.0         |0.0           |0.0         |0.0              |125.42857   |77.4375         |189.41667       |0.0               |0.0               |0.0             |0.0               |0.0               |0.0             |16.0            |1239.0          |12.0            |2273.0          |5808.0           |234.0            |7.0              |20.0            |0.0        |0.0       |0.0       |0.0       |0.0        |0.0      |0.0        |0.0        |0           |502.47943115234375     |4.006100177764893          |\n",
            "|0.0     |0.0     |1.1264048E8  |3.0         |0.0         |0.0            |0.0            |0.0            |0.0            |0.0             |0.0            |0.0            |0.0            |0.0             |0.0            |5.632024E7   |203.64676   |5.6320384E7 |5.6320096E7 |1.1264048E8 |5.632024E7  |203.64676  |5.6320384E7|5.6320096E7|0.0        |0.0         |0.0        |0.0        |0.0        |0.0          |0.0          |0.0          |0.0          |0.0           |0.0           |0.02663341    |0.0           |0.0        |0.0        |0.0         |0.0        |0.0        |0.0         |0.0         |0.0         |0.0         |0.0         |0.0         |0.0           |0.0         |0.0              |0.0         |0.0             |0.0             |0.0               |0.0               |0.0             |0.0               |0.0               |0.0             |3.0             |0.0             |0.0             |0.0             |0.0              |0.0              |0.0              |0.0             |0.0        |0.0       |0.0       |0.0       |5.632024E7 |203.64676|5.6320384E7|5.6320096E7|0           |0.0                    |0.026633409783244133       |\n",
            "|0.0     |0.0     |1.12641248E8 |3.0         |0.0         |0.0            |0.0            |0.0            |0.0            |0.0             |0.0            |0.0            |0.0            |0.0             |0.0            |5.6320624E7  |62.225395   |5.6320664E7 |5.6320576E7 |1.12641248E8|5.6320624E7 |62.225395  |5.6320664E7|5.6320576E7|0.0        |0.0         |0.0        |0.0        |0.0        |0.0          |0.0          |0.0          |0.0          |0.0           |0.0           |0.02663323    |0.0           |0.0        |0.0        |0.0         |0.0        |0.0        |0.0         |0.0         |0.0         |0.0         |0.0         |0.0         |0.0           |0.0         |0.0              |0.0         |0.0             |0.0             |0.0               |0.0               |0.0             |0.0               |0.0               |0.0             |3.0             |0.0             |0.0             |0.0             |0.0              |0.0              |0.0              |0.0             |0.0        |0.0       |0.0       |0.0       |5.6320624E7|62.225395|5.6320664E7|5.6320576E7|0           |0.0                    |0.026633229106664658       |\n",
            "|80.0    |6.0     |476513.0     |5.0         |3.0         |211.0          |463.0          |211.0          |0.0            |42.2            |94.36207       |463.0          |0.0            |154.33333       |267.31317      |68073.29     |115865.79   |237711.0    |24.0        |476513.0    |119128.25   |137379.97  |238470.0   |108.0      |238634.0   |119317.0    |167621.08  |237843.0   |791.0      |0.0          |0.0          |0.0          |0.0          |168.0         |104.0         |10.492893     |6.295736      |0.0        |463.0      |74.888885   |161.40588  |26051.861  |0.0         |0.0         |0.0         |1.0         |0.0         |0.0         |0.0           |0.0         |0.0              |84.25       |42.2            |154.33333       |0.0               |0.0               |0.0             |0.0               |0.0               |0.0             |5.0             |211.0           |3.0             |463.0           |14600.0          |219.0            |1.0              |32.0            |0.0        |0.0       |0.0       |0.0       |0.0        |0.0      |0.0        |0.0        |0           |1414.4420166015625     |16.78862953186035          |\n",
            "|80.0    |6.0     |475048.0     |5.0         |3.0         |220.0          |472.0          |220.0          |0.0            |44.0            |98.38699       |472.0          |0.0            |157.33333       |272.50934      |67864.0      |115746.93   |237494.0    |15.0        |475048.0    |118762.0    |137096.77  |237853.0   |15.0       |237516.0   |118758.0    |167472.58  |237179.0   |337.0      |0.0          |0.0          |0.0          |0.0          |168.0         |104.0         |10.525252     |6.315151      |0.0        |472.0      |76.888885   |165.06699  |27247.111  |0.0         |0.0         |0.0         |1.0         |0.0         |0.0         |0.0           |0.0         |0.0              |86.5        |44.0            |157.33333       |0.0               |0.0               |0.0             |0.0               |0.0               |0.0             |5.0             |220.0           |3.0             |472.0           |14600.0          |219.0            |1.0              |32.0            |0.0        |0.0       |0.0       |0.0       |0.0        |0.0      |0.0        |0.0        |0           |1456.6949462890625     |16.840402603149414         |\n",
            "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-------------+------------+------------+------------+------------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-----------------+------------+----------------+----------------+------------------+------------------+----------------+------------------+------------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+-----------+---------+-----------+-----------+------------+-----------------------+---------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.drop('Flow_Byts_s_imputed', 'Flow_Pkts_per_s')\n",
        "\n",
        "df.show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 7278:==========================================>             (3 + 1) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-----------------+------------+----------------+----------------+------------------+------------------+----------------+------------------+------------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+--------+--------+--------+------------+-----------------------+---------------------------+\n",
            "|Dst_Port|Protocol|Flow_Duration|Tot_Fwd_Pkts|Tot_Bwd_Pkts|TotLen_Fwd_Pkts|TotLen_Bwd_Pkts|Fwd_Pkt_Len_Max|Fwd_Pkt_Len_Min|Fwd_Pkt_Len_Mean|Fwd_Pkt_Len_Std|Bwd_Pkt_Len_Max|Bwd_Pkt_Len_Min|Bwd_Pkt_Len_Mean|Bwd_Pkt_Len_Std|Flow_IAT_Mean|Flow_IAT_Std|Flow_IAT_Max|Flow_IAT_Min|Fwd_IAT_Tot|Fwd_IAT_Mean|Fwd_IAT_Std|Fwd_IAT_Max|Fwd_IAT_Min|Bwd_IAT_Tot|Bwd_IAT_Mean|Bwd_IAT_Std|Bwd_IAT_Max|Bwd_IAT_Min|Fwd_PSH_Flags|Bwd_PSH_Flags|Fwd_URG_Flags|Bwd_URG_Flags|Fwd_Header_Len|Bwd_Header_Len|Fwd_Pkts_per_s|Bwd_Pkts_per_s|Pkt_Len_Min|Pkt_Len_Max|Pkt_Len_Mean|Pkt_Len_Std|Pkt_Len_Var|FIN_Flag_Cnt|SYN_Flag_Cnt|RST_Flag_Cnt|PSH_Flag_Cnt|ACK_Flag_Cnt|URG_Flag_Cnt|CWE_Flag_Count|ECE_Flag_Cnt|Down_per_Up_Ratio|Pkt_Size_Avg|Fwd_Seg_Size_Avg|Bwd_Seg_Size_Avg|Fwd_Byts_per_b_Avg|Fwd_Pkts_per_b_Avg|Fwd_Blk_Rate_Avg|Bwd_Byts_per_b_Avg|Bwd_Pkts_per_b_Avg|Bwd_Blk_Rate_Avg|Subflow_Fwd_Pkts|Subflow_Fwd_Byts|Subflow_Bwd_Pkts|Subflow_Bwd_Byts|Init_Fwd_Win_Byts|Init_Bwd_Win_Byts|Fwd_Act_Data_Pkts|Fwd_Seg_Size_Min|Active_Mean|Active_Std|Active_Max|Active_Min|Idle_Mean|Idle_Std|Idle_Max|Idle_Min|Label_binary|Flow_Byts_s_nan_imputed|Flow_Pkts_per_s_nan_imputed|\n",
            "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-----------------+------------+----------------+----------------+------------------+------------------+----------------+------------------+------------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+--------+--------+--------+------------+-----------------------+---------------------------+\n",
            "|       0|       0|            0|           0|           0|              0|              0|              0|              0|               0|              0|              0|              0|               0|              0|            0|           0|           0|           0|          0|           0|          0|          0|          0|          0|           0|          0|          0|          0|            0|            0|            0|            0|             0|             0|             0|             0|          0|          0|           0|          0|          0|           0|           0|           0|           0|           0|           0|             0|           0|                0|           0|               0|               0|                 0|                 0|               0|                 0|                 0|               0|               0|               0|               0|               0|                0|                0|                0|               0|          0|         0|         0|         0|        0|       0|       0|       0|           0|                      0|                          0|\n",
            "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-----------------+------------+----------------+----------------+------------------+------------------+----------------+------------------+------------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+--------+--------+--------+------------+-----------------------+---------------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Check for missing values again\n",
        "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------------+\n",
            "|            features|Label_binary|\n",
            "+--------------------+------------+\n",
            "|(78,[2,3,15,16,17...|           0|\n",
            "|(78,[2,3,15,16,17...|           0|\n",
            "|(78,[2,3,15,16,17...|           0|\n",
            "|(78,[0,1,2,3,4,5,...|           0|\n",
            "|(78,[0,1,2,3,4,5,...|           0|\n",
            "+--------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# VectorAssembler and the rest of the pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "feature_columns = [column for column in df.columns if column not in ['Label_binary']]  # Exclude the target column\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\") # VectorAssembler\n",
        "\n",
        "data = vectorAssembler.transform(df) # Transform the DataFrame\n",
        "\n",
        "data = data.select('features', 'Label_binary')\n",
        "data.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 7282:==========================================>             (3 + 1) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-----------------+------------+----------------+----------------+------------------+------------------+----------------+------------------+------------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+--------+--------+--------+------------+-----------------------+---------------------------+\n",
            "|Dst_Port|Protocol|Flow_Duration|Tot_Fwd_Pkts|Tot_Bwd_Pkts|TotLen_Fwd_Pkts|TotLen_Bwd_Pkts|Fwd_Pkt_Len_Max|Fwd_Pkt_Len_Min|Fwd_Pkt_Len_Mean|Fwd_Pkt_Len_Std|Bwd_Pkt_Len_Max|Bwd_Pkt_Len_Min|Bwd_Pkt_Len_Mean|Bwd_Pkt_Len_Std|Flow_IAT_Mean|Flow_IAT_Std|Flow_IAT_Max|Flow_IAT_Min|Fwd_IAT_Tot|Fwd_IAT_Mean|Fwd_IAT_Std|Fwd_IAT_Max|Fwd_IAT_Min|Bwd_IAT_Tot|Bwd_IAT_Mean|Bwd_IAT_Std|Bwd_IAT_Max|Bwd_IAT_Min|Fwd_PSH_Flags|Bwd_PSH_Flags|Fwd_URG_Flags|Bwd_URG_Flags|Fwd_Header_Len|Bwd_Header_Len|Fwd_Pkts_per_s|Bwd_Pkts_per_s|Pkt_Len_Min|Pkt_Len_Max|Pkt_Len_Mean|Pkt_Len_Std|Pkt_Len_Var|FIN_Flag_Cnt|SYN_Flag_Cnt|RST_Flag_Cnt|PSH_Flag_Cnt|ACK_Flag_Cnt|URG_Flag_Cnt|CWE_Flag_Count|ECE_Flag_Cnt|Down_per_Up_Ratio|Pkt_Size_Avg|Fwd_Seg_Size_Avg|Bwd_Seg_Size_Avg|Fwd_Byts_per_b_Avg|Fwd_Pkts_per_b_Avg|Fwd_Blk_Rate_Avg|Bwd_Byts_per_b_Avg|Bwd_Pkts_per_b_Avg|Bwd_Blk_Rate_Avg|Subflow_Fwd_Pkts|Subflow_Fwd_Byts|Subflow_Bwd_Pkts|Subflow_Bwd_Byts|Init_Fwd_Win_Byts|Init_Bwd_Win_Byts|Fwd_Act_Data_Pkts|Fwd_Seg_Size_Min|Active_Mean|Active_Std|Active_Max|Active_Min|Idle_Mean|Idle_Std|Idle_Max|Idle_Min|Label_binary|Flow_Byts_s_nan_imputed|Flow_Pkts_per_s_nan_imputed|\n",
            "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-----------------+------------+----------------+----------------+------------------+------------------+----------------+------------------+------------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+--------+--------+--------+------------+-----------------------+---------------------------+\n",
            "|       0|       0|            0|           0|           0|              0|              0|              0|              0|               0|              0|              0|              0|               0|              0|            0|           0|           0|           0|          0|           0|          0|          0|          0|          0|           0|          0|          0|          0|            0|            0|            0|            0|             0|             0|             0|             0|          0|          0|           0|          0|          0|           0|           0|           0|           0|           0|           0|             0|           0|                0|           0|               0|               0|                 0|                 0|               0|                 0|                 0|               0|               0|               0|               0|               0|                0|                0|                0|               0|          0|         0|         0|         0|        0|       0|       0|       0|           0|                      0|                          0|\n",
            "+--------+--------+-------------+------------+------------+---------------+---------------+---------------+---------------+----------------+---------------+---------------+---------------+----------------+---------------+-------------+------------+------------+------------+-----------+------------+-----------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+-----------+-----------+------------+-----------+-----------+------------+------------+------------+------------+------------+------------+--------------+------------+-----------------+------------+----------------+----------------+------------------+------------------+----------------+------------------+------------------+----------------+----------------+----------------+----------------+----------------+-----------------+-----------------+-----------------+----------------+-----------+----------+----------+----------+---------+--------+--------+--------+------------+-----------------------+---------------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Check again for any null or NaN values that might have been introduced\n",
        "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Label_binary|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "|(78,[2,3,15,16,17,18,19,20,21,22,23,35,60,72,73,74,75,77],[1.1264172E8,3.0,5.632086E7,139.30003356933594,5.632096E7,5.632076E7,1.1264172E8,5.632086E7,139.30003356933594,5.632096E7,5.632076E7,0.026633115485310555,3.0,5.632086E7,139.30003356933594,5.632096E7,5.632076E7,0.026633115485310555])                                                                                                                                                                                                                                                                                                                                                                                   |0           |\n",
            "|(78,[2,3,15,16,17,18,19,20,21,22,23,35,60,72,73,74,75,77],[1.12641464E8,3.0,5.6320732E7,114.55130004882812,5.6320816E7,5.6320652E7,1.12641464E8,5.6320732E7,114.55130004882812,5.6320816E7,5.6320652E7,0.02663317695260048,3.0,5.6320732E7,114.55130004882812,5.6320816E7,5.6320652E7,0.02663317695260048])                                                                                                                                                                                                                                                                                                                                                                          |0           |\n",
            "|(78,[2,3,15,16,17,18,19,20,21,22,23,35,60,72,73,74,75,77],[1.12638624E8,3.0,5.6319312E7,301.9346008300781,5.6319524E7,5.6319096E7,1.12638624E8,5.6319312E7,301.9346008300781,5.6319524E7,5.6319096E7,0.026633847504854202,3.0,5.6319312E7,301.9346008300781,5.6319524E7,5.6319096E7,0.026633847504854202])                                                                                                                                                                                                                                                                                                                                                                           |0           |\n",
            "|(78,[0,1,2,3,4,5,6,7,9,10,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,33,34,35,36,38,39,40,41,45,51,52,53,60,61,62,63,64,65,66,67,76,77],[22.0,6.0,6453966.0,15.0,10.0,1239.0,2273.0,744.0,82.5999984741211,196.74124145507812,976.0,227.3000030517578,371.6778869628906,268915.25,247443.78125,673900.0,22.0,6453966.0,460997.5625,123109.421875,673900.0,229740.0,5637902.0,626433.5625,455082.21875,1167293.0,554.0,488.0,328.0,2.3241522312164307,1.5494349002838135,976.0,135.07691955566406,277.8347473144531,77192.15625,1.0,140.47999572753906,82.5999984741211,227.3000030517578,15.0,1239.0,10.0,2273.0,65535.0,233.0,6.0,32.0,544.1614990234375,3.873587131500244])|0           |\n",
            "|(78,[0,1,2,3,4,5,6,7,9,10,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,33,34,35,36,38,39,40,41,45,51,52,53,60,61,62,63,64,65,66,67,76,77],[22.0,6.0,8804066.0,14.0,11.0,1143.0,2209.0,744.0,81.64286041259766,203.74554443359375,976.0,200.81817626953125,362.2498779296875,366836.09375,511356.625,1928102.0,21.0,8804066.0,677235.875,532417.0,1928102.0,246924.0,7715481.0,771548.125,755543.0625,2174893.0,90.0,456.0,360.0,1.5901743173599243,1.2494226694107056,976.0,128.92308044433594,279.7630310058594,78267.3515625,1.0,134.0800018310547,81.64286041259766,200.81817626953125,14.0,1143.0,11.0,2209.0,5808.0,233.0,6.0,32.0,380.7331848144531,2.83959698677063])   |0           |\n",
            "|(78,[0,1,2,3,4,5,6,7,9,10,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,33,34,35,36,38,39,40,41,45,51,52,53,60,61,62,63,64,65,66,67,76,77],[22.0,6.0,6989341.0,16.0,12.0,1239.0,2273.0,744.0,77.4375,190.83114624023438,976.0,189.4166717529297,347.642578125,258864.484375,291724.15625,951098.0,20.0,6989341.0,465956.0625,244363.890625,951098.0,265831.0,5980598.0,543690.75,460713.53125,1254338.0,78.0,332.0,252.0,2.2892000675201416,1.716900110244751,976.0,121.10344696044922,265.70867919921875,70601.09375,1.0,125.42857360839844,77.4375,189.4166717529297,16.0,1239.0,12.0,2273.0,5808.0,234.0,7.0,20.0,502.47943115234375,4.006100177764893])                     |0           |\n",
            "|(78,[2,3,15,16,17,18,19,20,21,22,23,35,60,72,73,74,75,77],[1.1264048E8,3.0,5.632024E7,203.64675903320312,5.6320384E7,5.6320096E7,1.1264048E8,5.632024E7,203.64675903320312,5.6320384E7,5.6320096E7,0.026633409783244133,3.0,5.632024E7,203.64675903320312,5.6320384E7,5.6320096E7,0.026633409783244133])                                                                                                                                                                                                                                                                                                                                                                             |0           |\n",
            "|(78,[2,3,15,16,17,18,19,20,21,22,23,35,60,72,73,74,75,77],[1.12641248E8,3.0,5.6320624E7,62.22539520263672,5.6320664E7,5.6320576E7,1.12641248E8,5.6320624E7,62.22539520263672,5.6320664E7,5.6320576E7,0.026633229106664658,3.0,5.6320624E7,62.22539520263672,5.6320664E7,5.6320576E7,0.026633229106664658])                                                                                                                                                                                                                                                                                                                                                                           |0           |\n",
            "|(78,[0,1,2,3,4,5,6,7,9,10,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,33,34,35,36,38,39,40,41,45,51,52,53,60,61,62,63,64,65,66,67,76,77],[80.0,6.0,476513.0,5.0,3.0,211.0,463.0,211.0,42.20000076293945,94.36206817626953,463.0,154.3333282470703,267.31317138671875,68073.2890625,115865.7890625,237711.0,24.0,476513.0,119128.25,137379.96875,238470.0,108.0,238634.0,119317.0,167621.078125,237843.0,791.0,168.0,104.0,10.49289321899414,6.295735836029053,463.0,74.88888549804688,161.4058837890625,26051.861328125,1.0,84.25,42.20000076293945,154.3333282470703,5.0,211.0,3.0,463.0,14600.0,219.0,1.0,32.0,1414.4420166015625,16.78862953186035])                       |0           |\n",
            "|(78,[0,1,2,3,4,5,6,7,9,10,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,33,34,35,36,38,39,40,41,45,51,52,53,60,61,62,63,64,65,66,67,76,77],[80.0,6.0,475048.0,5.0,3.0,220.0,472.0,220.0,44.0,98.38699340820312,472.0,157.3333282470703,272.50933837890625,67864.0,115746.9296875,237494.0,15.0,475048.0,118762.0,137096.765625,237853.0,15.0,237516.0,118758.0,167472.578125,237179.0,337.0,168.0,104.0,10.525252342224121,6.315151214599609,472.0,76.88888549804688,165.06698608398438,27247.111328125,1.0,86.5,44.0,157.3333282470703,5.0,220.0,3.0,472.0,14600.0,219.0,1.0,32.0,1456.6949462890625,16.840402603149414])                                                      |0           |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sparse Vectors\n",
        "data.show(10, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.00000000e+00 0.00000000e+00 1.12641720e+08 3.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.63208600e+07\n",
            " 1.39300034e+02 5.63209600e+07 5.63207600e+07 1.12641720e+08\n",
            " 5.63208600e+07 1.39300034e+02 5.63209600e+07 5.63207600e+07\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.66331155e-02\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 3.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 5.63208600e+07 1.39300034e+02 5.63209600e+07 5.63207600e+07\n",
            " 0.00000000e+00 2.66331155e-02]\n",
            "[0.00000000e+00 0.00000000e+00 1.12641464e+08 3.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.63207320e+07\n",
            " 1.14551300e+02 5.63208160e+07 5.63206520e+07 1.12641464e+08\n",
            " 5.63207320e+07 1.14551300e+02 5.63208160e+07 5.63206520e+07\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.66331770e-02\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 3.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 5.63207320e+07 1.14551300e+02 5.63208160e+07 5.63206520e+07\n",
            " 0.00000000e+00 2.66331770e-02]\n"
          ]
        }
      ],
      "source": [
        "# Extract the values from Sparse Vectors to the list format\n",
        "data_select = data.select('features').limit(2).collect()\n",
        "for val in data_select:\n",
        "    dense_vector = val[0].toArray()\n",
        "    print(dense_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "|scaledFeatures                                                                                                                                                                                                                                                                                                                                                                                                                    |Label_binary|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "|(78,[2,3,15,16,17,18,19,20,21,22,23,35,60,72,73,74,75,77],[4.2522159860802535,0.06744829905434724,5.920000923893201,1.8996483761697185E-7,0.037001223807810756,5.970040674556255,4.2801170875518935,5.863306444695915,1.8996449538682143E-7,0.037001263135200635,5.926192120326504,8.632359721661521E-8,0.06744829905434724,0.1040132089434138,3.6465683402046674E-7,0.037001726800782904,3.099633852791837,4.826841619062112E-8])|0           |\n",
            "|(78,[2,3,15,16,17,18,19,20,21,22,23,35,60,72,73,74,75,77],[4.25220632210058,0.06744829905434724,5.919987469551093,1.5621474421079272E-7,0.037001129204021536,5.9700292264793315,4.280107360161594,5.863293119202929,1.5621446278294493E-7,0.037001168531310864,5.926180756333031,8.632379644518764E-8,0.06744829905434724,0.10401297255336676,2.998700957810066E-7,0.03700163219570765,3.0996279089719008,4.8268527590608623E-8]) |0           |\n",
            "|(78,[2,3,15,16,17,18,19,20,21,22,23,35,60,72,73,74,75,77],[4.252099112326081,0.06744829905434724,5.919838210443332,4.117512103044961E-7,0.03700028039780162,5.969864289371069,4.279999446925462,5.86314528951511,4.117504685162767E-7,0.03700031972418878,5.926017030649301,8.632596984779595E-8,0.06744829905434724,0.1040103501012824,7.903983423315306E-7,0.0370007833779491,3.099542273936526,4.826974286319955E-8])          |0           |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Normalizing feature vectors\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
        "scaler_model = scaler.fit(data)\n",
        "data = scaler_model.transform(data)\n",
        "\n",
        "data = data.select(\"scaledFeatures\", \"Label_binary\")\n",
        "data.show(3, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- scaledFeatures: vector (nullable = true)\n",
            " |-- Label_binary: integer (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and testing sets\n",
        "train_data, test_data = data.randomSplit([0.7, 0.3], seed = 42)\n",
        "\n",
        "train_data.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 7291:>                                                       (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "|scaledFeatures                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Label_binary|\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "|(78,[0,1,2,3,4,5,6,7,8,9,11,12,13,15,17,18,33,34,35,36,37,38,39,40,41,50,51,52,53,60,61,62,63,67,76,77],[0.004638785395917926,3.8111254264379117,2.0913456011577777E-5,0.02248276635144908,0.009535780976760675,0.019065274350089624,0.0021715867569717124,1.0428568992913967,15.398208984746763,5.626530990341172,0.6661824950737462,7.069455923608339,2.082282832139368,5.8232074436307135E-5,3.6396180018108993E-7,5.872439458743393E-5,0.0074203286332193245,0.003802346364165736,0.005850564937979593,0.006759727153978902,15.867397115614759,0.654879068647082,3.074103297574771,0.11348055178629116,0.0023945841226460296,1.769024371094983,4.417415475458047,5.626530990341172,2.082282832139368,0.02248276635144908,0.019065274350089624,0.009535780976760675,0.0021715867569717124,0.7232061990314348,0.3667156154672587,0.006542764956099391])|0           |\n",
            "|(78,[0,1,2,3,4,5,6,7,8,9,11,12,13,15,17,18,33,34,35,36,37,38,39,40,41,50,51,52,53,60,61,62,63,67,76,77],[0.004638785395917926,3.8111254264379117,2.1932703867737705E-5,0.02248276635144908,0.009535780976760675,0.019065274350089624,0.0021715867569717124,1.0428568992913967,15.398208984746763,5.626530990341172,0.6661824950737462,7.069455923608339,2.082282832139368,6.107009972471922E-5,3.8170001065922967E-7,6.158641381822944E-5,0.0074203286332193245,0.003802346364165736,0.00557867971959316,0.006445591696467522,15.867397115614759,0.654879068647082,3.074103297574771,0.11348055178629116,0.0023945841226460296,1.769024371094983,4.417415475458047,5.626530990341172,2.082282832139368,0.02248276635144908,0.019065274350089624,0.009535780976760675,0.0021715867569717124,0.7232061990314348,0.34967375527832883,0.006238712082949931]) |0           |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "train_data.show(2, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#! Double check for any Negative, Infinity or NaN values\n",
        "\n",
        "# Iterate through each column in the DataFrame\n",
        "for column in df.columns:\n",
        "    # Filter rows where the column contains problematic values\n",
        "    problematic_rows = df.filter(\n",
        "        (col(column).isNull()) |   # Check for null values\n",
        "        (isnan(col(column))) |     # Check for NaN values\n",
        "        (col(column) < 0) |        # Check for negative values\n",
        "        (col(column) == float('inf'))  # Check for infinity values\n",
        "    )\n",
        "    \n",
        "    # If there are any rows containing problematic values, print the column name\n",
        "    if problematic_rows.count() > 0:\n",
        "        print(f\"Column '{column}' has {problematic_rows.count()} rows with problematic values.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRGHM3rOsKvU"
      },
      "source": [
        "---\n",
        "# **Task 2 - Model Selection and Implementation (25 marks)**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1st student name: Philip Acquaye-Mensah\n",
        "Model: Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "6oAJuyTBsRO-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Apply Logistic Regression\n",
        "lr = LogisticRegression(featuresCol = \"scaledFeatures\", labelCol = 'Label_binary', threshold = 0.5, regParam = 0.01)\n",
        "\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "lr_predictions_train = lr_model.transform(train_data)\n",
        "lr_predictions_test = lr_model.transform(test_data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 7561:>                                                       (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+----------+\n",
            "|Label_binary|prediction|\n",
            "+------------+----------+\n",
            "|           0|       0.0|\n",
            "|           0|       0.0|\n",
            "|           0|       0.0|\n",
            "|           0|       0.0|\n",
            "|           0|       0.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "|           1|       1.0|\n",
            "+------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "lr_predictions_test.select(\"Label_binary\", \"prediction\").show(20) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Area Under ROC = 0.999922\n",
            "Area Under ROC :  99.992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Area Under PR = 0.999922\n",
            "Area Under PR =  99.971\n"
          ]
        }
      ],
      "source": [
        "# Check for accuracy\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Evaluate AUC (Area Under ROC)\n",
        "auc_evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"Label_binary\",\n",
        "    rawPredictionCol=\"rawPrediction\",  # ensure logistic regression is configured to provide rawPrediction\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "auc = auc_evaluator.evaluate(lr_predictions_test)\n",
        "print(\"Area Under ROC = %g\" % auc)\n",
        "print(\"Area Under ROC : \" , round(auc, 5)*100)\n",
        "\n",
        "# Evaluate AUPRC (Area Under Precision-Recall Curve)\n",
        "pr_evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"Label_binary\",\n",
        "    rawPredictionCol=\"rawPrediction\",  # ensure logistic regression is configured to provide rawPrediction\n",
        "    metricName=\"areaUnderPR\"\n",
        ")\n",
        "pr = pr_evaluator.evaluate(lr_predictions_test)\n",
        "print(\"Area Under PR = %g\" % auc)\n",
        "print(\"Area Under PR = \",  round(pr, 5)*100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2nd Student name: Mohamed Jareer Mohamed Zeenam\n",
        "Model: Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Apply Naive Bayes Classifier\n",
        "\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "\n",
        "# Initialize Naive Bayes model\n",
        "nb = NaiveBayes(featuresCol='scaledFeatures', labelCol='Label_binary', smoothing=0.5)  \n",
        "\n",
        "# Fit the model to the training data\n",
        "nb_model = nb.fit(train_data)\n",
        "\n",
        "# Make predictions on the test data\n",
        "nb_predictions = nb_model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 7590:==========================================>             (3 + 1) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+----------+\n",
            "|Label_binary|prediction|\n",
            "+------------+----------+\n",
            "|           0|       0.0|\n",
            "|           1|       1.0|\n",
            "|           0|       1.0|\n",
            "|           1|       0.0|\n",
            "+------------+----------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# 2nd student name: Mohamed Jareer Mohamed Zeenam\n",
        "\n",
        "nb_predictions.select('Label_binary', 'prediction').distinct().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBh-tqvEHmIa"
      },
      "source": [
        "---\n",
        "# **Task 3 - Model Parameter Tuning (20 marks)**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common Evaluation Metrics for Binary Classification\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Define the evaluator for binary classification\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='Label_binary', metricName='areaUnderROC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "mhLRVi66Hmdo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coefficients: (78,[0,2,13,19,24,25,26,27,35,36,39,47,50,51,53,64,65,66,67,68,70,76,77],[-0.7377382206214376,-0.18638754785032724,-0.08028669859168529,-0.11657393557833604,-0.13480223104795874,-0.06517840137466267,-0.019978900371638672,-0.037072229878835634,0.10116820408726121,0.569461353224445,-0.050537381427365455,0.8734253023812084,0.05722981629779612,-0.11235983046531868,-0.08028669859168529,0.4654817852470462,-0.4041570571654708,0.8357422642166366,2.620223547279337,-0.002388870943457007,-0.04890605342349861,-0.09466145607096989,0.28741967073059466])\n",
            "Intercept: -8.131112552751372\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 9984:==========================================>             (3 + 1) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC:  0.9985469860176518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "##1st student name: Philip Acquaye-Mensah\n",
        "# perform grid search to find the best hyperparameters for the lr model \n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "paramGrid = (ParamGridBuilder()\n",
        "                .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
        "                .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "                .addGrid(lr.maxIter, [1, 5, 10])\n",
        "                .build())\n",
        "\n",
        "# Create 5-fold CrossValidator\n",
        "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
        "\n",
        "# Run cross validations\n",
        "cvModel = cv.fit(train_data)\n",
        "\n",
        "# Use test set here so we can measure the accuracy of our model on new data\n",
        "lr_predictions = cvModel.transform(test_data)\n",
        "\n",
        "# cvModel uses the best model found from the Cross Validation\n",
        "# Evaluate best model\n",
        "evaluator.evaluate(lr_predictions)\n",
        "\n",
        "# Print the coefficients and intercept for logistic regression\n",
        "print(\"Coefficients: \" + str(cvModel.bestModel.coefficients))\n",
        "print(\"Intercept: \" + str(cvModel.bestModel.intercept))\n",
        "\n",
        "# Print the AUC for the test set\n",
        "print(\"AUC: \", evaluator.evaluate(lr_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 11128:=========================================>             (3 + 1) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best smoothing parameter: 0.1\n",
            "Area under ROC (AUC): 94.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# 2nd student name: Mohamed Jareer Mohamed Zeenam\n",
        "# Grid Search results for Naive Bayes\n",
        "\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# Initialize Naive Bayes model\n",
        "nb = NaiveBayes(featuresCol='scaledFeatures', labelCol='Label_binary')\n",
        "\n",
        "# Define a grid of hyperparameters to search over\n",
        "param_grid = ParamGridBuilder() \\\n",
        "              .addGrid(nb.smoothing, [0.1, 0.15, 0.2, 0.27, 0.3, 0.33, 0.4, 0.48, 0.5, 0.6, 0.7, 0.72, 0.76, 0.8, 0.9, 1]) \\\n",
        "              .build()\n",
        "\n",
        "# Define cross-validation\n",
        "crossval = CrossValidator(estimator=nb,\n",
        "                           estimatorParamMaps=param_grid,\n",
        "                           evaluator=evaluator,\n",
        "                           numFolds=5)\n",
        "\n",
        "# Fit the models and select the best one using cross-validation\n",
        "cv_model = crossval.fit(train_data)\n",
        "\n",
        "# Get the best model from cross-validation\n",
        "best_nb_model = cv_model.bestModel\n",
        "\n",
        "# Make predictions on the test data using the best model\n",
        "nb_predictions = best_nb_model.transform(test_data)\n",
        "\n",
        "# Evaluate the best model\n",
        "auc = evaluator.evaluate(nb_predictions)\n",
        "\n",
        "# Print the best hyperparameters and the evaluation metric\n",
        "print(\"Best smoothing parameter:\", best_nb_model.getOrDefault(\"smoothing\"))\n",
        "print(\"Area under ROC (AUC):\", round(auc, 2)*100, \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 11198:=========================================>             (3 + 1) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best smoothing value:  0.2\n",
            "Area under ROC curve: 94.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# 2nd student name: Mohamed Jareer Mohamed Zeenam\n",
        "# Random Search results for Naive Bayes\n",
        "\n",
        "import random\n",
        "\n",
        "# Perform random search manually\n",
        "best_auc_roc = 0.0\n",
        "best_params = None\n",
        "\n",
        "for smoothing in random.sample([0.1, 0.15, 0.2, 0.27, 0.3, 0.33, 0.4, 0.48, 0.5, 0.6, 0.7, 0.72, 0.76, 0.8, 0.9, 1], 5):\n",
        "    nb = NaiveBayes(featuresCol='scaledFeatures', labelCol='Label_binary', smoothing=smoothing)\n",
        "    model = nb.fit(train_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions_rand_search = model.transform(test_data)\n",
        "\n",
        "    # Evaluate\n",
        "    accuracy_rand_search = evaluator.evaluate(predictions_rand_search)\n",
        "\n",
        "    # Check if the current model is the best so far\n",
        "    if accuracy_rand_search > best_auc_roc:\n",
        "        best_auc_roc = accuracy_rand_search\n",
        "        best_params = smoothing\n",
        "\n",
        "\n",
        "# Print the results\n",
        "print(\"Best smoothing value: \", best_params)\n",
        "print(\"Area under ROC curve:\", round(best_auc_roc, 2) * 100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 11215:=========================================>             (3 + 1) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+----------+------+\n",
            "|Label_binary|prediction|count |\n",
            "+------------+----------+------+\n",
            "|0           |0.0       |177834|\n",
            "|1           |1.0       |114293|\n",
            "|0           |1.0       |22699 |\n",
            "|1           |0.0       |3     |\n",
            "+------------+----------+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "cm = nb_predictions.groupBy('Label_binary', 'prediction').count()\n",
        "cm.show(cm.count(), truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np9gJPEIHwbG"
      },
      "source": [
        "---\n",
        "# **Task 4 - Model Evaluation and Accuracy Calculation (20 marks)**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "FCVAx1VsHws-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Metrics:\n",
            "Accuracy:  100.0\n",
            "Precision:  99.0\n",
            "Recall:  100.0\n",
            "ROC AUC:  100.0\n",
            "F1 Score:  100.0\n",
            "Confusion Matrix :  [[199952    581]\n",
            " [     1 114295]]\n"
          ]
        }
      ],
      "source": [
        "##1st student name: Philip Acquaye-Mensah\n",
        "# import numpy to check for accuracy_score, precision_score, recall_score, roc_auc_score, f1_score & confusion_matrix\n",
        "# Best to use sklearn metrics\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix \n",
        "\n",
        "# Calculate the Logistic Regression metrics\n",
        "accuracy_lr = accuracy_score(labels, predictions)\n",
        "precision_lr = precision_score(labels, predictions)\n",
        "recall_lr = recall_score(labels, predictions)\n",
        "roc_auc_lr = roc_auc_score(labels, predictions)\n",
        "f1_lr = f1_score(labels, predictions)\n",
        "\n",
        "# Print the Logistic Regression metrics\n",
        "print(\"Logistic Regression Metrics:\")\n",
        "print(\"Accuracy: \", round(accuracy_lr, 2) * 100)\n",
        "print(\"Precision: \", round(precision_lr, 2) * 100)\n",
        "print(\"Recall: \", round(recall_lr, 2) * 100)\n",
        "print(\"ROC AUC: \", round(roc_auc_lr, 2) * 100)\n",
        "print(\"F1 Score: \", round(f1_lr, 2) * 100)\n",
        "\n",
        "\n",
        "# Confusion Matrix to check misclassification\n",
        "ConfusionMatrix = confusion_matrix(labels, predictions)\n",
        "print(\"Confusion Matrix : \" , ConfusionMatrix) # show the confusion matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 93.0\n",
            "f1_score: 91.0\n",
            "precision: 83.0\n",
            "recall: 100.0\n",
            "AUC: 94.0\n"
          ]
        }
      ],
      "source": [
        "# 2nd student name: Mohamed Jareer Mohamed Zeenam\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
        "\n",
        "# Make predictions\n",
        "predictionAndTarget = nb_model.transform(test_data).select(\"Label_binary\", \"prediction\")\n",
        "\n",
        "predictionAndTargetNumpy = np.array((predictionAndTarget.collect()))\n",
        "\n",
        "acc = accuracy_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
        "f1 = f1_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
        "precision = precision_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
        "recall = recall_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
        "auc = roc_auc_score(predictionAndTargetNumpy[:,0], predictionAndTargetNumpy[:,1])\n",
        "\n",
        "# print the evaluation metrics\n",
        "print(\"Accuracy:\", round(acc, 2) * 100)\n",
        "print(\"f1_score:\", round(f1, 2) * 100)\n",
        "print(\"precision:\", round(precision, 2) * 100)\n",
        "print(\"recall:\", round(recall, 2) * 100)\n",
        "print(\"AUC:\", round(auc, 2) * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 11225:=========================================>             (3 + 1) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+----------+------+\n",
            "|Label_binary|prediction|count |\n",
            "+------------+----------+------+\n",
            "|0           |0.0       |177834|\n",
            "|1           |1.0       |114293|\n",
            "|0           |1.0       |22699 |\n",
            "|1           |0.0       |3     |\n",
            "+------------+----------+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# 2nd student name: Mohamed Jareer Mohamed Zeenam\n",
        "# Confusion Matrix\n",
        "\n",
        "cm = nb_predictions.groupBy('Label_binary', 'prediction').count()\n",
        "cm.show(cm.count(), truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 11236:=========================================>             (3 + 1) / 4]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------+------+\n",
            "|Label_binary|   0.0|   1.0|\n",
            "+------------+------+------+\n",
            "|           1|     3|114293|\n",
            "|           0|177834| 22699|\n",
            "+------------+------+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# 2nd student name: Mohamed Jareer Mohamed Zeenam\n",
        "\n",
        "#  Check the Missclassification rate\n",
        "confusion_matrix = cm.groupBy('Label_binary').pivot('prediction').sum('count').na.fill(0)\n",
        "confusion_matrix.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsERZv2WIB1H"
      },
      "source": [
        "---\n",
        "# **Task 5 - Results Visualization or Printing (5 marks)**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "j04CcL30ILx_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/17 00:37:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3gAAAHUCAYAAABoL4ryAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABO1ElEQVR4nO3dZ3RUVf/28WsgyaSRQEINhNA7oYhUJYSAECCCgBSRIsWCiIqIIjdVioUmKKLSOyLCDYhSIvUGEZAgGKRJFUGkJBApCdnPC57M3yEBkhBIOH4/a81aZs8+5/zOYWecK/vMHpsxxggAAAAA8NDLltkFAAAAAAAyBgEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPQJY1Y8YM2Ww27dix44Eet169eqpXr16atomOjtaQIUN09OjRZM916dJFRYoUyZDahgwZIpvN5ni4urqqcOHC6tGjh06fPp0hx3gYZOQ1Tauka9+lS5cUnx82bJijT0rjIb3u5ZxTO6br1avnNL48PDxUqVIljR8/XomJiek69u3s2rVLISEh8vX1lc1m0/jx4zN0//82RYoUkc1mu+2/86xZsxz/ruvXr0/z/u/0Gncn6Xk9BXBvXDK7AADIaiZNmpTmbaKjozV06FDVq1cv2ZvwgQMH6tVXX82g6m767rvv5Ovrq8uXL2v16tUaM2aMtmzZoqioKLm6umbosbKi+3FN0yJHjhxatGiRJk6cqBw5cjjajTGaMWOGfHx8FBsbm2n13YtixYpp7ty5kqQ///xTkydP1uuvv64//vhD77//foYdp2vXroqLi9OCBQuUK1euTAvsVpIjRw5t3LhRhw8fVvHixZ2emzZt2j2Nyzu9xt1Jel5PAdwbZvAA4BblypVTuXLlMmx/xYsXV5UqVTJsf5L0yCOPqGbNmmrQoIE++OADde7cWdHR0dq8eXOGHudujDG6cuXKAz2mdH+uaVo0b95cxhgtWLDAqf3777/XkSNH1LZt20yq7N55eHioZs2aqlmzpp588kn997//VbFixfTxxx8rPj7+nvZ948YNXbt2TZK0d+9eNWjQQOHh4apZs6by589/T/uOj49XQkLCPe3jYffYY4+pYMGCmjZtmlP74cOHtXHjxgc6Lv/++29JGf96CuDuCHgAHnqbN29WWFiYcuTIIU9PT9WuXVvffPNNiv1q1aold3d3FSxYUAMHDtSUKVOS3UqX0i1Fn376qSpVqiRvb2/lyJFDZcqU0TvvvCPp5q2kTz/9tCQpNDTUcRvUjBkzJKV8a11iYqImTpyoypUry8PDQzlz5lTNmjW1bNmydF2DatWqSZLOnDnj1L527VqFhYXJx8dHnp6eqlOnjiIjI5Nt/9///lfBwcGy2+0qVqyYPvroI8ftoP9ks9nUq1cvTZ48WWXLlpXdbtfMmTMlSQcPHtQzzzyjvHnzym63q2zZsvrkk0+Snffw4cNVunRpx3kHBwfro48+cvQ5e/asnn/+eQUGBsputytPnjyqU6eO1q5d6+iT0jW9evWq+vfvr6JFi8rNzU0FCxbUyy+/rIsXLzr1K1KkiJo1a6bvvvtOVatWlYeHh8qUKZPsTfGd+Pr66qmnnkq2zbRp01SnTh2VKlUqxe2mTZumSpUqyd3dXX5+fnrqqae0b9++ZP1mzJih0qVLO67jrFmzUtzf9evXNXz4cJUpU8ZxrZ577jmdPXs21edyN66urnrkkUf0999/O/Z7+vRpvfDCCypUqJDc3NxUtGhRDR061ClgHT16VDabTR988IGGDx+uokWLym63a/r06bLZbEpISNCnn37q+H1JsnfvXjVv3ly5cuWSu7u7Kleu7BhjSdavXy+bzabZs2frjTfeUMGCBWW323Xo0CF16dJF3t7e+vXXX9WoUSN5eXmpQIECeu+99yRJP/zwgx577DF5eXmpVKlSyfZ99uxZ9ezZU+XKlZO3t7fy5s2r+vXra9OmTU79ks5v9OjRGjt2rIoWLSpvb2/VqlVLP/zwQ7LruG3bNkVERMjf31/u7u4qXry4XnvtNac+qfkdupNs2bKpU6dOmjlzptMttdOmTVNgYKAaNGiQ4nY7duzQk08+KT8/P7m7u6tKlSr68ssvHc/f7TWuXr16qlChgjZu3KjatWvL09NTXbt2dTx36+vptWvXNGzYMJUtW1bu7u7y9/dXaGiotmzZ4uizaNEi1ahRQ76+vvL09FSxYsUc+wRwZ9yiCeChtmHDBjVs2FDBwcGaOnWq7Ha7Jk2apIiICM2fP9/xF+uff/5ZDRs2dLyh8/T01OTJkzVnzpy7HmPBggXq2bOnXnnlFY0ePVrZsmXToUOHFB0dLUlq2rSpRo4cqXfeeUeffPKJqlatKknJbpH6py5dumjOnDnq1q2bhg0bJjc3N/3000/p/szWkSNHJMkpWMyZM0edOnVS8+bNNXPmTLm6uuqzzz5To0aNtGrVKoWFhUm6ebtny5YtVbduXS1cuFAJCQkaPXp0srCYZOnSpdq0aZMGDRqk/PnzK2/evIqOjlbt2rVVuHBhjRkzRvnz59eqVavUu3dv/fXXXxo8eLAk6YMPPtCQIUP0n//8R3Xr1lV8fLx+/fVXpxDWsWNH/fTTTxoxYoRKlSqlixcv6qefftK5c+due/7GGLVo0UKRkZHq37+/Hn/8cf38888aPHiwtm7dqq1bt8putzv67969W2+88Ybefvtt5cuXT1OmTFG3bt1UokQJ1a1bN1XXvFu3bgoLC9O+fftUtmxZXbx4UV9//bUmTZqUYq2jRo3SO++8o/bt22vUqFE6d+6chgwZolq1amn79u0qWbKkpJtvpp977jk1b95cY8aMUUxMjIYMGaJr164pW7b/+7tsYmKimjdvrk2bNqlfv36qXbu2jh07psGDB6tevXrasWOHPDw8UnUud3P48GG5uLgoV65cOn36tKpXr65s2bJp0KBBKl68uLZu3arhw4fr6NGjmj59utO2EyZMUKlSpTR69Gj5+PgoZ86c2rp1q2rVqqXWrVvrjTfecPTdv3+/ateurbx582rChAny9/fXnDlz1KVLF505c0b9+vVz2nf//v1Vq1YtTZ48WdmyZVPevHkl3ZzNa9mypV588UW9+eabmjdvnvr376/Y2FgtXrxYb731lgoVKqSJEyeqS5cuqlChgh555BFJ0vnz5yVJgwcPVv78+XX58mUtWbJE9erVU2RkZLKw8sknn6hMmTKOzxAOHDhQTZo00ZEjR+Tr6ytJWrVqlSIiIlS2bFmNHTtWhQsX1tGjR7V69WrHflL7O3Q3Xbt21ahRo7Rq1SqFh4frxo0bmjlzprp16+Y0fpKsW7dOjRs3Vo0aNTR58mT5+vpqwYIFatu2rf7++2916dIlVa9xf/zxh5599ln169dPI0eOTPFYkpSQkKDw8HBt2rRJr732murXr6+EhAT98MMPOn78uGrXrq2tW7eqbdu2atu2rYYMGSJ3d3cdO3ZM33//faquAfCvZwAgi5o+fbqRZLZv337bPjVr1jR58+Y1ly5dcrQlJCSYChUqmEKFCpnExERjjDFPP/208fLyMmfPnnX0u3HjhilXrpyRZI4cOeJoDwkJMSEhIY6fe/XqZXLmzHnHWhctWmQkmXXr1iV7rnPnziYoKMjx88aNG40kM2DAgDvuMyWDBw82kszp06dNfHy8uXDhgvnyyy+Nl5eXad++vaNfXFyc8fPzMxEREU7b37hxw1SqVMlUr17d0fboo4+awMBAc+3aNUfbpUuXjL+/v7n1fxOSjK+vrzl//rxTe6NGjUyhQoVMTEyMU3uvXr2Mu7u7o3+zZs1M5cqV73iO3t7e5rXXXrtjn1uv6XfffWckmQ8++MCp38KFC40k8/nnnzvagoKCjLu7uzl27Jij7cqVK8bPz8+88MILdzyuMTevwcsvv2wSExNN0aJFTd++fY0xxnzyySfG29vbXLp0yXz44YdO4+rChQvGw8PDNGnSxGlfx48fN3a73TzzzDPGmJv/PgEBAaZq1aqOsWuMMUePHjWurq5O5zx//nwjySxevNhpn9u3bzeSzKRJkxxtt47p2wkJCTHly5c38fHxJj4+3pw6dcq8/fbbRpJ5+umnjTHGvPDCC8bb29vp+hljzOjRo40k88svvxhjjDly5IiRZIoXL26uX79+2+v4T+3atTN2u90cP37cqT08PNx4enqaixcvGmOMWbdunZFk6tatm2y/nTt3TnZd4uPjTZ48eYwk89NPPznaz507Z7Jnz2769Olz22uSkJBg4uPjTVhYmHnqqacc7UnnV7FiRZOQkOBo//HHH40kM3/+fEdb8eLFTfHixc2VK1due5zU/g7dTlBQkGnatKkx5ua/Y+vWrY0xxnzzzTfGZrOZI0eOpPg6VaZMGVOlShUTHx/vtL9mzZqZAgUKmBs3bhhj7vwaFxISYiSZyMjIFJ/759ibNWuWkWS++OKL255L0lhK+vcGkDbcogngoRUXF6dt27apdevW8vb2drRnz55dHTt21MmTJ7V//35JN2f66tevr9y5czv6ZcuWTW3atLnrcapXr66LFy+qffv2+u9//6u//vrrnur+9ttvJUkvv/xyuveRP39+ubq6KleuXGrTpo0eeeQRp1vNtmzZovPnz6tz585KSEhwPBITE9W4cWNt375dcXFxiouL044dO9SiRQu5ubk5tvf29lZERESKx65fv75y5crl+Pnq1auKjIzUU089JU9PT6fjNWnSRFevXnXcsla9enXt3r1bPXv21KpVq1Jc8KF69eqaMWOGhg8frh9++CFVn/tK+sv+rStbPv300/Ly8kp2W2rlypVVuHBhx8/u7u4qVaqUjh07dtdjJUlaSXP27NlKSEjQ1KlT1aZNG6exmGTr1q26cuVKsvoCAwNVv359R3379+/XqVOn9MwzzzjdthgUFKTatWs7bbtixQrlzJlTERERTte8cuXKyp8/f7pWSpSkX375Ra6urnJ1dVVAQIDGjBmjDh066IsvvnAcNzQ0VAEBAU7HDQ8Pl3Tzd+2fnnzyyVQv/PP9998rLCxMgYGBTu1dunTR33//ra1btzq1t2rVKsX92Gw2NWnSxPGzi4uLSpQooQIFCjh9dtPPz0958+ZN9u8+efJkVa1aVe7u7nJxcZGrq6siIyNTvJ22adOmyp49u+Pn4OBgSXLs88CBAzp8+LC6desmd3f3FOtNy+9QanTt2lXLli3TuXPnNHXqVIWGhqa4MMqhQ4f066+/qkOHDpKU7Lh//PGH4zX0bnLlyqX69evftd+3334rd3f3O95u+eijj0qS2rRpoy+//FK///57qmoAcBMBD8BD68KFCzLGqECBAsmeCwgIkCTHrXLnzp1Tvnz5kvVLqe1WHTt21LRp03Ts2DG1atVKefPmVY0aNbRmzZp01X327Fllz579nhaVWLt2rbZv365Vq1apVatW2rhxo1555RXH80m3V7Zu3drxZj3p8f7778sYo/PnzzuuYVquza3X+9y5c0pISNDEiROTHSvpTXZSKO7fv79Gjx6tH374QeHh4fL391dYWJjTV2EsXLhQnTt31pQpU1SrVi35+fmpU6dOd/waiHPnzsnFxUV58uRxarfZbMqfP3+yWyb9/f2T7cNut6d5wZikz7uNHDlSP/30k7p163bb+qTk1066OVb/OU4lpTg2bm07c+aMLl68KDc3t2TX/fTp0+n+Q0Tx4sW1fft27dixQ3v37tXFixc1Z84cx+2GZ86c0fLly5Mds3z58pKU7LgpnfPtnDt3LlW/z3fbt6enZ7Iw5ebmJj8/v2R93dzcdPXqVcfPY8eO1UsvvaQaNWpo8eLF+uGHH7R9+3Y1btw4xfFx61hKuhU4qW/S5xYLFSqUYq1J55Xa36HUaN26tdzd3TVu3DgtX778tuMy6XWib9++yY7bs2fPNB03tf/OZ8+eVUBAwG1v4ZSkunXraunSpUpISFCnTp1UqFAhVahQQfPnz0/VMYB/Oz6DB+ChlStXLmXLlk1//PFHsudOnTolSY4ZO39//xQ/U5ba74577rnn9NxzzykuLk4bN27U4MGD1axZMx04cEBBQUFpqjtPnjy6ceOGTp8+naY3v/9UqVIlx7k1bNhQjRo10ueff65u3brp0UcfdTw3ceJE1axZM8V95MuXT/Hx8bLZbGm6NrcuvJIrVy7HrOntZiWLFi0q6eZMSp8+fdSnTx9dvHhRa9eu1TvvvKNGjRrpxIkT8vT0VO7cuTV+/HiNHz9ex48f17Jly/T222/rzz//1HfffZfi/v39/ZWQkKCzZ886hTxjjE6fPu2YEchoSQtXDB06VKVLl042y/bP+iTddqz+c5xKKV/7W9ty584tf3//216Tf359Q1q4u7s7Fu1JSe7cuRUcHKwRI0ak+HxSGEty63i5E39//1T9Pqdn36k1Z84c1atXT59++qlT+6VLl9K1v6TxePLkydv2ScvvUGp4enqqXbt2GjVqlHx8fNSyZcsU+yVdz/79+9+2T+nSpVN1zNT+W+TJk0ebN29WYmLiHUNe8+bN1bx5c127dk0//PCDRo0apWeeeUZFihRRrVq1UnUs4N+KgAfgoeXl5aUaNWro66+/1ujRox0LSiQmJmrOnDkqVKiQY9GRkJAQrVy5Un/99ZfjTU1iYqIWLVqU5mOGh4fr+vXratGihX755RcFBQUl+6v9nYSHh2vUqFH69NNPNWzYsDQdPyU2m02ffPKJypUrp//85z9atWqV6tSpo5w5cyo6Olq9evW67bZubm6qVq2ali5dqtGjRztu07x8+bJWrFiRquN7enoqNDRUu3btUnBwsNOtnneSM2dOtW7dWr///rtee+01HT16NNly6oULF1avXr0UGRmp//3vf7fdV1hYmD744APNmTNHr7/+uqN98eLFiouLcywocz+88cYb8vDwcKwymJJatWrJw8NDc+bMcep38uRJff/992rdurWkm2+mCxQooPnz56tPnz6ON83Hjh3Tli1bnMJTs2bNtGDBAt24cUM1atS4T2eXXLNmzbRy5UoVL17c6VbdjBAWFqYlS5bo1KlTTuc6a9YseXp63vaPFRnJZrM5Lcgj3VykaevWrcluHU2NUqVKqXjx4po2bZr69OmTbN9S+n+H7uSll17SmTNnFBIScttbQ0uXLq2SJUtq9+7dGjly5B33l5bXuDsJDw/X/PnzNWPGjFStimm32xUSEqKcOXNq1apV2rVrFwEPuAsCHoAs7/vvv09xdckmTZpo1KhRatiwoUJDQ9W3b1+5ublp0qRJ2rt3r+bPn+94gzxgwAAtX75cYWFhGjBggDw8PDR58mTFxcVJ0h3/ktyjRw95eHioTp06KlCggE6fPq1Ro0bJ19fXMTNUoUIFSdLnn3+uHDlyyN3dXUWLFk3xVsDHH39cHTt21PDhw3XmzBk1a9ZMdrtdu3btkqenp9OtlqlVsmRJPf/885o0aZI2b96sxx57TBMnTlTnzp11/vx5tW7dWnnz5tXZs2e1e/dunT171jFDMWzYMDVt2lSNGjXSq6++qhs3bujDDz+Ut7e3Y0XBu/noo4/02GOP6fHHH9dLL72kIkWK6NKlSzp06JCWL1/u+IxcRESEKlSooGrVqilPnjw6duyYxo8fr6CgIJUsWVIxMTEKDQ3VM888ozJlyihHjhzavn27Y6XP20maxXzrrbcUGxurOnXqOFbRrFKlijp27Jjma5paTzzxhJ544ok79smZM6cGDhyod955R506dVL79u117tw5DR06VO7u7o4VErNly6Z3331X3bt311NPPaUePXro4sWLGjJkSLJbNNu1a6e5c+eqSZMmevXVV1W9enW5urrq5MmTWrdunZo3b66nnnoqw8932LBhWrNmjWrXrq3evXurdOnSunr1qo4ePaqVK1dq8uTJd7wd8U4GDx7s+IzfoEGD5Ofnp7lz5+qbb77RBx984LhN9H5q1qyZ3n33XQ0ePFghISHav3+/hg0bpqJFi6b7e/Y++eQTRUREqGbNmnr99ddVuHBhHT9+XKtWrXJ8qXxqf4dSq3Llylq6dOld+3322WcKDw9Xo0aN1KVLFxUsWFDnz5/Xvn379NNPPzn+CJaW17g7ad++vaZPn64XX3xR+/fvV2hoqBITE7Vt2zaVLVtW7dq106BBg3Ty5EmFhYWpUKFCunjxoj766CO5uroqJCQkTccD/pUyd40XALi9pFU0b/dIWqFw06ZNpn79+sbLy8t4eHiYmjVrmuXLlyfb36ZNm0yNGjWM3W43+fPnN2+++aZ5//33k63WduuqbzNnzjShoaEmX758xs3NzQQEBJg2bdqYn3/+2Wn/48ePN0WLFjXZs2c3ksz06dONMclXfDTm5mqJ48aNMxUqVDBubm7G19fX1KpVK8W6/ylpFc1/rgaa5MyZM8bb29uEhoY62jZs2GCaNm1q/Pz8jKurqylYsKBp2rSpWbRokdO2S5YsMRUrVjRubm6mcOHC5r333jO9e/c2uXLlcuqnFFY+THLkyBHTtWtXU7BgQePq6mry5MljateubYYPH+7oM2bMGFO7dm2TO3dux7G6detmjh49aowx5urVq+bFF180wcHBxsfHx3h4eJjSpUubwYMHm7i4OMd+UrqmV65cMW+99ZYJCgoyrq6upkCBAuall14yFy5ccOr3z9UG/ym1K03e6RokuXUVzSRTpkwxwcHBjn/z5s2bO1advLVfyZIljZubmylVqpSZNm1aiuccHx9vRo8ebSpVqmTc3d2Nt7e3KVOmjHnhhRfMwYMH03xuSato3s3Zs2dN7969TdGiRY2rq6vx8/MzjzzyiBkwYIC5fPmyMeb/Vpn88MMPU9zH7a7jnj17TEREhPH19TVubm6mUqVKjt+lJEmraN46jo25OTa8vLxSfW63jodr166Zvn37moIFCxp3d3dTtWpVs3Tp0mTX/07nJ8kMHjzYqW3r1q0mPDzc+Pr6GrvdbooXL25ef/11pz6p+R26nduN63+63UqYu3fvNm3atDF58+Y1rq6uJn/+/KZ+/fpm8uTJTv1u9xp3p3GT0ti7cuWKGTRokGOM+/v7m/r165stW7YYY4xZsWKFCQ8PNwULFjRubm4mb968pkmTJmbTpk13vQ4AjLEZY8yDjZQAkHU88cQTOnr0qA4cOJDZpWQp8fHxqly5sgoWLOj0XV0AACBr4xZNAP8affr0UZUqVRQYGKjz589r7ty5WrNmjaZOnZrZpWW6bt26qWHDho5bUCdPnqx9+/bpo48+yuzSAABAGhDwAPxr3LhxQ4MGDdLp06dls9lUrlw5zZ49W88++2xml5bpLl26pL59++rs2bNydXVV1apVtXLlSjVo0CCzSwMAAGnALZoAAAAAYBF80TkAAAAAWAQBDwAAAAAsgoAHAAAAABbBIitZWGJiok6dOqUcOXI4vqwZAAAAwL+PMUaXLl1SQECAsmW7/TwdAS8LO3XqlAIDAzO7DAAAAABZxIkTJ1SoUKHbPk/Ay8Jy5Mgh6eY/oo+PTyZXAwAAACCzxMbGKjAw0JERboeAl4Ul3Zbp4+NDwAMAAABw149uscgKAAAAAFgEAQ8AAAAALIKABwAAAAAWQcADAAAAAItgkZWHwNMRQ+XqYs/sMgAAAADLWxE5MrNLuCfM4AEAAACARRDwAAAAAMAiCHgAAAAAYBEEPAAAAACwCAIeAAAAAFgEAQ8AAAAALIKABwAAAAAWQcADAAAAAIsg4AEAAACARRDwAAAAAMAiCHgAAAAAYBEEPAAAAACwCAIeAAAAAFgEAQ8AAAAALIKABwAAAAAWQcADAAAAAIsg4AEAAACARRDwAAAAAMAiCHgAAAAAYBEEPAAAAACwCAIeAAAAAFgEAQ8AAAAALIKABwAAAAAWQcADAAAAAIsg4AEAAACARRDwAAAAAMAiskTA27Jli7Jnz67GjRtndikAAAAA8NDKEgFv2rRpeuWVV7R582YdP3480+qIj4/PtGMDAAAAwL3K9IAXFxenL7/8Ui+99JKaNWumGTNmOD2/bNkyVatWTe7u7sqdO7datmzpeO7atWvq16+fAgMDZbfbVbJkSU2dOlWSNGPGDOXMmdNpX0uXLpXNZnP8PGTIEFWuXFnTpk1TsWLFZLfbZYzRd999p8cee0w5c+aUv7+/mjVrpsOHDzvt6+TJk2rXrp38/Pzk5eWlatWqadu2bTp69KiyZcumHTt2OPWfOHGigoKCZIzJgKsGAAAAAMllesBbuHChSpcurdKlS+vZZ5/V9OnTHSHom2++UcuWLdW0aVPt2rVLkZGRqlatmmPbTp06acGCBZowYYL27dunyZMny9vbO03HP3TokL788kstXrxYUVFRkm6Gzj59+mj79u2KjIxUtmzZ9NRTTykxMVGSdPnyZYWEhOjUqVNatmyZdu/erX79+ikxMVFFihRRgwYNNH36dKfjTJ8+XV26dHEKmLe6du2aYmNjnR4AAAAAkFoumV3A1KlT9eyzz0qSGjdurMuXLysyMlINGjTQiBEj1K5dOw0dOtTRv1KlSpKkAwcO6Msvv9SaNWvUoEEDSVKxYsXSfPzr169r9uzZypMnj6OtVatWyWrMmzevoqOjVaFCBc2bN09nz57V9u3b5efnJ0kqUaKEo3/37t314osvauzYsbLb7dq9e7eioqL09ddf37GWUaNGOZ0rAAAAAKRFps7g7d+/Xz/++KPatWsnSXJxcVHbtm01bdo0SVJUVJTCwsJS3DYqKkrZs2dXSEjIPdUQFBTkFO4k6fDhw3rmmWdUrFgx+fj4qGjRopLk+HxgVFSUqlSp4gh3t2rRooVcXFy0ZMkSSTc/YxgaGqoiRYrcsZb+/fsrJibG8Thx4sQ9nRsAAACAf5dMncGbOnWqEhISVLBgQUebMUaurq66cOGCPDw8brvtnZ6TpGzZsiX7vFtKi6h4eXkla4uIiFBgYKC++OILBQQEKDExURUqVND169dTdWw3Nzd17NhR06dPV8uWLTVv3jyNHz/+jttIkt1ul91uv2s/AAAAAEhJps3gJSQkaNasWRozZoyioqIcj927dysoKEhz585VcHCwIiMjU9y+YsWKSkxM1IYNG1J8Pk+ePLp06ZLi4uIcbUmfsbuTc+fOad++ffrPf/6jsLAwlS1bVhcuXHDqExwcrKioKJ0/f/62++nevbvWrl2rSZMmKT4+3mlxGAAAAAC4HzJtBm/FihW6cOGCunXrJl9fX6fnWrduralTp2rcuHEKCwtT8eLF1a5dOyUkJOjbb79Vv379VKRIEXXu3Fldu3bVhAkTVKlSJR07dkx//vmn2rRpoxo1asjT01PvvPOOXnnlFf3444/JVuhMSa5cueTv76/PP/9cBQoU0PHjx/X222879Wnfvr1GjhypFi1aaNSoUSpQoIB27dqlgIAA1apVS5JUtmxZ1axZU2+99Za6du1611k/AAAAALhXmTaDN3XqVDVo0CBZuJNuLnISFRUlHx8fLVq0SMuWLVPlypVVv359bdu2zdHv008/VevWrdWzZ0+VKVNGPXr0cMzY+fn5ac6cOVq5cqUqVqyo+fPna8iQIXetK1u2bFqwYIF27typChUq6PXXX9eHH37o1MfNzU2rV69W3rx51aRJE1WsWFHvvfeesmfP7tSvW7duun79urp27ZqOKwQAAAAAaWMzfDHbfTNixAgtWLBAe/bsSdf2sbGx8vX11RN1+8jVhc/mAQAAAPfbisiRmV1CipKyQUxMjHx8fG7bL9O/B8+KLl++rO3bt2vixInq3bt3ZpcDAAAA4F+CgHcf9OrVS4899phCQkK4PRMAAADAA5PpX3RuRTNmzEjVgi4AAAAAkJGYwQMAAAAAiyDgAQAAAIBFEPAAAAAAwCIIeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsgoAHAAAAABZBwAMAAAAAiyDgAQAAAIBFEPAAAAAAwCIIeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsgoAHAAAAABZBwAMAAAAAiyDgAQAAAIBFEPAAAAAAwCIIeAAAAABgEQQ8AAAAALAIl8wuAHe3aPlg+fj4ZHYZAAAAALI4ZvAAAAAAwCIIeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsgoAHAAAAABZBwAMAAAAAiyDgAQAAAIBFEPAAAAAAwCIIeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsgoAHAAAAABbhktkF4O4avfq+XNzcM7sMAAAAwLI2fTYws0vIEMzgAQAAAIBFEPAAAAAAwCIIeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsgoAHAAAAABZBwAMAAAAAiyDgAQAAAIBFEPAAAAAAwCIIeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsgoAHAAAAABZBwAMAAAAAiyDgAQAAAIBFEPAAAAAAwCIIeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsgoAHAAAAABZBwAMAAAAAiyDgAQAAAIBF/GsDXpEiRTR+/PgM7wsAAAAAmcUlswuQpC5dumjmzJmSJBcXFwUGBqply5YaOnSovLy87ssxt2/fnup9p6UvAAAAAGSWLBHwJKlx48aaPn264uPjtWnTJnXv3l1xcXH69NNPnfrFx8fL1dX1no+XJ0+e+9IXAAAAADJLlrlF0263K3/+/AoMDNQzzzyjDh06aOnSpRoyZIgqV66sadOmqVixYrLb7TLGKCYmRs8//7zy5s0rHx8f1a9fX7t373ba57Jly1StWjW5u7srd+7catmypeO5W2+7HDJkiAoXLiy73a6AgAD17t37tn2PHz+u5s2by9vbWz4+PmrTpo3OnDnjtK/KlStr9uzZKlKkiHx9fdWuXTtdunQp4y8cAAAAAPx/WSbg3crDw0Px8fGSpEOHDunLL7/U4sWLFRUVJUlq2rSpTp8+rZUrV2rnzp2qWrWqwsLCdP78eUnSN998o5YtW6pp06batWuXIiMjVa1atRSP9dVXX2ncuHH67LPPdPDgQS1dulQVK1ZMsa8xRi1atND58+e1YcMGrVmzRocPH1bbtm2d+h0+fFhLly7VihUrtGLFCm3YsEHvvffeHc/52rVrio2NdXoAAAAAQGplmVs0/+nHH3/UvHnzFBYWJkm6fv26Zs+e7bhV8vvvv9eePXv0559/ym63S5JGjx6tpUuX6quvvtLzzz+vESNGqF27dho6dKhjv5UqVUrxeMePH1f+/PnVoEEDubq6qnDhwqpevXqKfdeuXauff/5ZR44cUWBgoCRp9uzZKl++vLZv365HH31UkpSYmKgZM2YoR44ckqSOHTsqMjJSI0aMuO15jxo1yqleAAAAAEiLLDODt2LFCnl7e8vd3V21atVS3bp1NXHiRElSUFCQ0+fgdu7cqcuXL8vf31/e3t6Ox5EjR3T48GFJUlRUlCMg3s3TTz+tK1euqFixYurRo4eWLFmihISEFPvu27dPgYGBjnAnSeXKlVPOnDm1b98+R1uRIkUc4U6SChQooD///POOdfTv318xMTGOx4kTJ1JVPwAAAABIWWgGLzQ0VJ9++qlcXV0VEBDgtJDKrStYJiYmqkCBAlq/fn2y/eTMmVPSzVs8UyswMFD79+/XmjVrtHbtWvXs2VMffvihNmzYkGxBF2OMbDZbsn3c2n7rdjabTYmJiXesw263O2YkAQAAACCtsswMnpeXl0qUKKGgoKC7rpJZtWpVnT59Wi4uLipRooTTI3fu3JKk4OBgRUZGpvr4Hh4eevLJJzVhwgStX79eW7du1Z49e5L1K1eunI4fP+40uxYdHa2YmBiVLVs21ccDAAAAgIyWZWbw0qJBgwaqVauWWrRooffff1+lS5fWqVOntHLlSrVo0ULVqlXT4MGDFRYWpuLFi6tdu3ZKSEjQt99+q379+iXb34wZM3Tjxg3VqFFDnp6emj17tjw8PBQUFJTisYODg9WhQweNHz9eCQkJ6tmzp0JCQm67iAsAAAAAPAhZZgYvLWw2m1auXKm6deuqa9euKlWqlNq1a6ejR48qX758kqR69epp0aJFWrZsmSpXrqz69etr27ZtKe4vZ86c+uKLL1SnTh3HzN/y5cvl7++f4rGXLl2qXLlyqW7dumrQoIGKFSumhQsX3tdzBgAAAIC7sRljTGYXgZTFxsbK19dXNbu8Ixc398wuBwAAALCsTZ8NzOwS7igpG8TExMjHx+e2/R7KGTwAAAAAQHIEPAAAAACwCAIeAAAAAFgEAQ8AAAAALIKABwAAAAAWQcADAAAAAIsg4AEAAACARRDwAAAAAMAiCHgAAAAAYBEEPAAAAACwCAIeAAAAAFgEAQ8AAAAALIKABwAAAAAWQcADAAAAAIsg4AEAAACARRDwAAAAAMAiCHgAAAAAYBEEPAAAAACwCAIeAAAAAFgEAQ8AAAAALIKABwAAAAAWQcADAAAAAIsg4AEAAACARRDwAAAAAMAiXDK7ANzdqo/eko+PT2aXAQAAACCLYwYPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARLpldAO6uzgejlN3dntllAAAAAP8aUf8ZktklpAszeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsgoAHAAAAABZBwAMAAAAAiyDgAQAAAIBFEPAAAAAAwCIIeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsIl0Bb/v27dq2bVuy9m3btmnHjh33XBQAAAAAIO3SFfBefvllnThxIln777//rpdffvmeiwIAAAAApF26Al50dLSqVq2arL1KlSqKjo6+56IAAAAAAGmXroBnt9t15syZZO1//PGHXFxc7rkoAAAAAEDapSvgNWzYUP3791dMTIyj7eLFi3rnnXfUsGHDDCsOAAAAAJB66ZpuGzNmjOrWraugoCBVqVJFkhQVFaV8+fJp9uzZGVogAAAAACB10hXwChYsqJ9//llz587V7t275eHhoeeee07t27eXq6trRtcIAAAAAEiFdH9gzsvLS88//3xG1gIAAAAAuAepDnjLli1TeHi4XF1dtWzZsjv2ffLJJ++5MAAAAABA2qQ64LVo0UKnT59W3rx51aJFi9v2s9lsunHjRkbUBgAAAABIg1QHvMTExBT/GwAAAACQNaT5axLi4+MVGhqqAwcO3I96AAAAAADplOaA5+rqqr1798pms92PegAAAAAA6ZSuLzrv1KmTpk6dmtG1WE6RIkU0fvx4x882m01Lly7NtHoAAAAAWFu6vibh+vXrmjJlitasWaNq1arJy8vL6fmxY8dmSHH3okuXLpo5c6YkKXv27AoICFDTpk01cuRI5cqVK5OrAwAAAICMl66At3fvXlWtWlWSsvRn8Ro3bqzp06crISFB0dHR6tq1qy5evKj58+dndmkAAAAAkOHSdYvmunXr7vjIKux2u/Lnz69ChQrpiSeeUNu2bbV69WrH89OnT1fZsmXl7u6uMmXKaNKkSU7bnzx5Uu3atZOfn5+8vLxUrVo1bdu2TZJ0+PBhNW/eXPny5ZO3t7ceffRRrV279oGeHwAAAAD8U7oCXteuXXXp0qVk7XFxcerates9F3U//Pbbb/ruu+/k6uoqSfriiy80YMAAjRgxQvv27dPIkSM1cOBAx22dly9fVkhIiE6dOqVly5Zp9+7d6tevn+MrIi5fvqwmTZpo7dq12rVrlxo1aqSIiAgdP3483TVeu3ZNsbGxTg8AAAAASK103aI5c+ZMvffee8qRI4dT+5UrVzRr1ixNmzYtQ4q7VytWrJC3t7du3Lihq1evSvq/zwe+++67GjNmjFq2bClJKlq0qKKjo/XZZ5+pc+fOmjdvns6ePavt27fLz89PklSiRAnHvitVqqRKlSo5fh4+fLiWLFmiZcuWqVevXumqd9SoURo6dGi6tgUAAACANAW82NhYGWNkjNGlS5fk7u7ueO7GjRtauXKl8ubNm+FFpldoaKg+/fRT/f3335oyZYoOHDigV155RWfPntWJEyfUrVs39ejRw9E/ISFBvr6+kqSoqChVqVLFEe5uFRcXp6FDh2rFihU6deqUEhISdOXKlXuawevfv7/69Onj+Dk2NlaBgYHp3h8AAACAf5c0BbycOXPKZrPJZrOpVKlSyZ632WxZagbKy8vLMes2YcIEhYaGaujQoY4Zti+++EI1atRw2iZ79uySJA8Pjzvu+80339SqVas0evRolShRQh4eHmrdurWuX7+e7nrtdrvsdnu6twcAAADw75amgLdu3ToZY1S/fn0tXrzYaXbLzc1NQUFBCggIyPAiM8rgwYMVHh6ul156SQULFtRvv/2mDh06pNg3ODhYU6ZM0fnz51Ocxdu0aZO6dOmip556StLNz+QdPXr0fpYPAAAAAHeUpoAXEhIiSTpy5IgKFy4sm812X4q6X+rVq6fy5ctr5MiRGjJkiHr37i0fHx+Fh4fr2rVr2rFjhy5cuKA+ffqoffv2GjlypFq0aKFRo0apQIEC2rVrlwICAlSrVi2VKFFCX3/9tSIiImSz2TRw4EDHAiwAAAAAkBnStYpmUFCQNm/erGeffVa1a9fW77//LkmaPXu2Nm/enKEFZrQ+ffroiy++UKNGjTRlyhTNmDFDFStWVEhIiGbMmKGiRYtKujkjuXr1auXNm1dNmjRRxYoV9d577zlu4Rw3bpxy5cql2rVrKyIiQo0aNXJ8NyAAAAAAZAabMcakdaPFixerY8eO6tChg2bPnq3o6GgVK1ZMkyZN0ooVK7Ry5cr7Ueu/TmxsrHx9fVVhwNvK7s5n8wAAAIAHJeo/QzK7BCdJ2SAmJkY+Pj637ZeuGbzhw4dr8uTJ+uKLLxzfKydJtWvX1k8//ZSeXQIAAAAA7lG6At7+/ftVt27dZO0+Pj66ePHivdYEAAAAAEiHdAW8AgUK6NChQ8naN2/erGLFit1zUQAAAACAtEtXwHvhhRf06quvatu2bbLZbDp16pTmzp2rvn37qmfPnhldIwAAAAAgFdL0NQlJ+vXrp5iYGIWGhurq1auqW7eu7Ha7+vbt6/gScQAAAADAg5WugCdJI0aM0IABAxQdHa3ExESVK1dO3t7eGVkbAAAAACAN0hTwunbtmqp+06ZNS1cxAAAAAID0S1PAmzFjhoKCglSlShWl4+vzAAAAAAD3UZoC3osvvqgFCxbot99+U9euXfXss8/Kz8/vftUGAAAAAEiDNK2iOWnSJP3xxx966623tHz5cgUGBqpNmzZatWoVM3oAAAAAkMnS/DUJdrtd7du315o1axQdHa3y5curZ8+eCgoK0uXLl+9HjQAAAACAVEjX9+AlsdlsstlsMsYoMTExo2oCAAAAAKRDmgPetWvXNH/+fDVs2FClS5fWnj179PHHH+v48eN8TQIAAAAAZKI0LbLSs2dPLViwQIULF9Zzzz2nBQsWyN/f/37VBgAAAABIgzQFvMmTJ6tw4cIqWrSoNmzYoA0bNqTY7+uvv86Q4gAAAAAAqZemgNepUyfZbLb7VQsAAAAA4B6k+YvOAQAAAABZ0z2togkAAAAAyDoIeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsgoAHAAAAABZBwAMAAAAAi0jT9+Ahc/yvX3/5+PhkdhkAAAAAsjhm8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFuGS2QXg7lp8NUQunvbMLgMAAAD4V1jdblRml5BuzOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIjI14HXp0kU2m002m00uLi4qXLiwXnrpJV24cCFZ3y1btqhJkybKlSuX3N3dVbFiRY0ZM0Y3btxI1nfdunVq0qSJ/P395enpqXLlyumNN97Q77//fteaRo4cqezZs+u9995L9tyQIUNUuXLlZO0XL16UzWbT+vXrndoXL16sevXqydfXV97e3goODtawYcN0/vz5u9YBAAAAAGmV6TN4jRs31h9//KGjR49qypQpWr58uXr27OnUZ8mSJQoJCVGhQoW0bt06/frrr3r11Vc1YsQItWvXTsYYR9/PPvtMDRo0UP78+bV48WJFR0dr8uTJiomJ0ZgxY+5az/Tp09WvXz9Nmzbtns5rwIABatu2rR599FF9++232rt3r8aMGaPdu3dr9uzZ97RvAAAAAEiJS2YXYLfblT9/fklSoUKF1LZtW82YMcPxfFxcnHr06KEnn3xSn3/+uaO9e/fuypcvn5588kl9+eWXatu2rU6ePKnevXurd+/eGjdunKNvkSJFVLduXV28ePGOtWzYsEFXrlzRsGHDNGvWLG3cuFF169ZN8zn9+OOPGjlypMaPH69XX33VqY6GDRvetQ4AAAAASI9Mn8H7p99++03fffedXF1dHW2rV6/WuXPn1Ldv32T9IyIiVKpUKc2fP1+StGjRIl2/fl39+vVLcf85c+a84/GnTp2q9u3by9XVVe3bt9fUqVPTdR5z586Vt7d3spnIu9Vx7do1xcbGOj0AAAAAILUyPeCtWLFC3t7e8vDwUPHixRUdHa233nrL8fyBAwckSWXLlk1x+zJlyjj6HDx4UD4+PipQoECa64iNjdXixYv17LPPSpKeffZZffXVV+kKWQcPHlSxYsWcgmpqjBo1Sr6+vo5HYGBgmo8NAAAA4N8r0wNeaGiooqKitG3bNr3yyitq1KiRXnnllWT9/vk5u1vbbTZbsv9Oq3nz5qlYsWKqVKmSJKly5coqVqyYFixYkOZ9pbeO/v37KyYmxvE4ceJEmvcBAAAA4N8r0wOel5eXSpQooeDgYE2YMEHXrl3T0KFDHc+XKlVKkrRv374Ut//1119VsmRJR9+YmBj98ccfaa5j2rRp+uWXX+Ti4uJ4/PLLL063afr4+CgmJibZtkmfqfP19XXUcfjwYcXHx6epBrvdLh8fH6cHAAAAAKRWpge8Ww0ePFijR4/WqVOnJElPPPGE/Pz8UlwBc9myZTp48KDat28vSWrdurXc3Nz0wQcfpLjv2y1usmfPHu3YsUPr169XVFSU47Fx40Zt375de/fulXTzdtCTJ0/q9OnTTttv375d2bJlU4kSJSRJzzzzjC5fvqxJkyalqQ4AAAAAuBeZvormrerVq6fy5ctr5MiR+vjjj+Xl5aXPPvtM7dq10/PPP69evXrJx8dHkZGRevPNN9W6dWu1adNGkhQYGKhx48apV69eio2NVadOnVSkSBGdPHlSs2bNkre3d4pBcerUqapevXqKK2bWqlVLU6dO1bhx4/TEE0+obNmyateunUaMGKGAgAD9/PPP6tu3r1588UXlyJFDklSjRg3169fP8d17Tz31lAICAnTo0CFNnjxZjz32mNPqmgAAAACQEbLcDJ4k9enTR1988YXjM2itW7fWunXrdOLECdWtW1elS5fW2LFjNWDAAC1YsMDp8249e/bU6tWrHcGqTJky6t69u3x8fFJcifP69euaM2eOWrVqlWItrVq10pw5c3T9+nW5uLho9erVKlasmDp06KDy5cvr7bffVvfu3TV27Fin7d5//33NmzdP27ZtU6NGjVS+fHn16dNHwcHB6ty5cwZeLQAAAAC4yWZut3oJMl1sbKx8fX0VOvV1uXjaM7scAAAA4F9hdbtRmV1CMknZICYm5o5rdWTJGTwAAAAAQNoR8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLcMnsAnB3S1sPkY+PT2aXAQAAACCLYwYPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARLpldAO5u0g9d5O7lmtllAAAAAJb3Wp2FmV3CPWEGDwAAAAAsgoAHAAAAABZBwAMAAAAAiyDgAQAAAIBFEPAAAAAAwCIIeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsgoAHAAAAABZBwAMAAAAAiyDgAQAAAIBFEPAAAAAAwCIIeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsgoAHAAAAABZBwAMAAAAAiyDgAQAAAIBFEPAAAAAAwCIIeAAAAABgEQQ8AAAAALAIAh4AAAAAWAQBDwAAAAAsgoAHAAAAABaRpQJely5dZLPZkj0OHTokSdq4caMiIiIUEBAgm82mpUuX3nWfN27c0KhRo1SmTBl5eHjIz89PNWvW1PTp0+/z2QAAAADAg+WS2QXcqnHjxsnCV548eSRJcXFxqlSpkp577jm1atUqVfsbMmSIPv/8c3388ceqVq2aYmNjtWPHDl24cCHDa09y/fp1ubm53bf9AwAAAEBKstQMniTZ7Xblz5/f6ZE9e3ZJUnh4uIYPH66WLVumen/Lly9Xz5499fTTT6to0aKqVKmSunXrpj59+jj6JCYm6v3331eJEiVkt9tVuHBhjRgxwvH8nj17VL9+fXl4eMjf31/PP/+8Ll++7Hi+S5cuatGihUaNGqWAgACVKlVKkvT777+rbdu2ypUrl/z9/dW8eXMdPXr0Hq8QAAAAAKQsywW8jJY/f359//33Onv27G379O/fX++//74GDhyo6OhozZs3T/ny5ZMk/f3332rcuLFy5cql7du3a9GiRVq7dq169erltI/IyEjt27dPa9as0YoVK/T3338rNDRU3t7e2rhxozZv3ixvb281btxY169fT7GOa9euKTY21ukBAAAAAKmV5W7RXLFihby9vR0/h4eHa9GiRene39ixY9W6dWvlz59f5cuXV+3atdW8eXOFh4dLki5duqSPPvpIH3/8sTp37ixJKl68uB577DFJ0ty5c3XlyhXNmjVLXl5ekqSPP/5YERERev/99x1B0MvLS1OmTHHcmjlt2jRly5ZNU6ZMkc1mkyRNnz5dOXPm1Pr16/XEE08kq3XUqFEaOnRous8VAAAAwL9blpvBCw0NVVRUlOMxYcKEe9pfuXLltHfvXv3www967rnndObMGUVERKh79+6SpH379unatWsKCwtLcft9+/apUqVKjnAnSXXq1FFiYqL279/vaKtYsaLT5+527typQ4cOKUeOHPL29pa3t7f8/Px09epVHT58OMVj9e/fXzExMY7HiRMn7uncAQAAAPy7ZLkZPC8vL5UoUSJD95ktWzY9+uijevTRR/X6669rzpw56tixowYMGCAPD487bmuMcczA3eqf7f8MgNLNz/U98sgjmjt3brLtkhaNuZXdbpfdbr/b6QAAAABAirLcDN6DUK5cOUk3V+UsWbKkPDw8FBkZedu+UVFRiouLc7T973//U7Zs2RyLqaSkatWqOnjwoPLmzasSJUo4PXx9fTP2hAAAAABAD1nAu3z5suPWTUk6cuSIoqKidPz48dtu07p1a40bN07btm3TsWPHtH79er388ssqVaqUypQpI3d3d7311lvq16+fZs2apcOHD+uHH37Q1KlTJUkdOnSQu7u7OnfurL1792rdunV65ZVX1LFjR8fn71LSoUMH5c6dW82bN9emTZt05MgRbdiwQa+++qpOnjyZodcFAAAAAKSHLODt2LFDVapUUZUqVSRJffr0UZUqVTRo0KDbbtOoUSMtX75cERERKlWqlDp37qwyZcpo9erVcnG5eYfqwIED9cYbb2jQoEEqW7as2rZtqz///FOS5OnpqVWrVun8+fN69NFH1bp1a4WFhenjjz++Y62enp7auHGjChcurJYtW6ps2bLq2rWrrly5Ih8fnwy6IgAAAADwf2zGGJPZRSBlsbGx8vX11ahVT8ndyzWzywEAAAAs77U6CzO7hBQlZYOYmJg7Thg9VDN4AAAAAIDbI+ABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFuGS2QXg7nrWnCEfH5/MLgMAAABAFscMHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIviahCzMGCNJio2NzeRKAAAAAGSmpEyQlBFuh4CXhZ07d06SFBgYmMmVAAAAAMgKLl26JF9f39s+T8DLwvz8/CRJx48fv+M/InA3sbGxCgwM1IkTJ+Tj45PZ5eAhx3hCRmEsISMxnpCRsuJ4Msbo0qVLCggIuGM/Al4Wli3bzY9I+vr6ZpmBhYebj48PYwkZhvGEjMJYQkZiPCEjZbXxlJpJHxZZAQAAAACLIOABAAAAgEUQ8LIwu92uwYMHy263Z3YpeMgxlpCRGE/IKIwlZCTGEzLSwzyebOZu62wCAAAAAB4KzOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCXiaaNGmSihYtKnd3dz3yyCPatGnTHftv2LBBjzzyiNzd3VWsWDFNnjz5AVWKh0FaxtPXX3+thg0bKk+ePPLx8VGtWrW0atWqB1gtsrq0vj4l+d///icXFxdVrlz5/haIh0Zax9K1a9c0YMAABQUFyW63q3jx4po2bdoDqhZZXVrH09y5c1WpUiV5enqqQIECeu6553Tu3LkHVC2yqo0bNyoiIkIBAQGy2WxaunTpXbd5mN6HE/AyycKFC/Xaa69pwIAB2rVrlx5//HGFh4fr+PHjKfY/cuSImjRposcff1y7du3SO++8o969e2vx4sUPuHJkRWkdTxs3blTDhg21cuVK7dy5U6GhoYqIiNCuXbsecOXIitI6npLExMSoU6dOCgsLe0CVIqtLz1hq06aNIiMjNXXqVO3fv1/z589XmTJlHmDVyKrSOp42b96sTp06qVu3bvrll1+0aNEibd++Xd27d3/AlSOriYuLU6VKlfTxxx+nqv9D9z7cIFNUr17dvPjii05tZcqUMW+//XaK/fv162fKlCnj1PbCCy+YmjVr3rca8fBI63hKSbly5czQoUMzujQ8hNI7ntq2bWv+85//mMGDB5tKlSrdxwrxsEjrWPr222+Nr6+vOXfu3IMoDw+ZtI6nDz/80BQrVsypbcKECaZQoUL3rUY8fCSZJUuW3LHPw/Y+nBm8THD9+nXt3LlTTzzxhFP7E088oS1btqS4zdatW5P1b9SokXbs2KH4+Pj7ViuyvvSMp1slJibq0qVL8vPzux8l4iGS3vE0ffp0HT58WIMHD77fJeIhkZ6xtGzZMlWrVk0ffPCBChYsqFKlSqlv3766cuXKgygZWVh6xlPt2rV18uRJrVy5UsYYnTlzRl999ZWaNm36IEqGhTxs78NdMruAf6O//vpLN27cUL58+Zza8+XLp9OnT6e4zenTp1Psn5CQoL/++ksFChS4b/Uia0vPeLrVmDFjFBcXpzZt2tyPEvEQSc94OnjwoN5++21t2rRJLi78bwU3pWcs/fbbb9q8ebPc3d21ZMkS/fXXX+rZs6fOnz/P5/D+5dIznmrXrq25c+eqbdu2unr1qhISEvTkk09q4sSJD6JkWMjD9j6cGbxMZLPZnH42xiRru1v/lNrx75TW8ZRk/vz5GjJkiBYuXKi8efPer/LwkEnteLpx44aeeeYZDR06VKVKlXpQ5eEhkpbXpsTERNlsNs2dO1fVq1dXkyZNNHbsWM2YMYNZPEhK23iKjo5W7969NWjQIO3cuVPfffedjhw5ohdffPFBlAqLeZjeh/On1kyQO3duZc+ePdlfnP78889kfx1Ikj9//hT7u7i4yN/f/77ViqwvPeMpycKFC9WtWzctWrRIDRo0uJ9l4iGR1vF06dIl7dixQ7t27VKvXr0k3XyTboyRi4uLVq9erfr16z+Q2pG1pOe1qUCBAipYsKB8fX0dbWXLlpUxRidPnlTJkiXva83IutIznkaNGqU6derozTfflCQFBwfLy8tLjz/+uIYPH57lZl2QdT1s78OZwcsEbm5ueuSRR7RmzRqn9jVr1qh27dopblOrVq1k/VevXq1q1arJ1dX1vtWKrC8940m6OXPXpUsXzZs3j88jwCGt48nHx0d79uxRVFSU4/Hiiy+qdOnSioqKUo0aNR5U6chi0vPaVKdOHZ06dUqXL192tB04cEDZsmVToUKF7mu9yNrSM57+/vtvZcvm/FY3e/bskv5v9gVIjYfufXgmLe7yr7dgwQLj6upqpk6daqKjo81rr71mvLy8zNGjR40xxrz99tumY8eOjv6//fab8fT0NK+//rqJjo42U6dONa6uruarr77KrFNAFpLW8TRv3jzj4uJiPvnkE/PHH384HhcvXsysU0AWktbxdCtW0USStI6lS5cumUKFCpnWrVubX375xWzYsMGULFnSdO/ePbNOAVlIWsfT9OnTjYuLi5k0aZI5fPiw2bx5s6lWrZqpXr16Zp0CsohLly6ZXbt2mV27dhlJZuzYsWbXrl3m2LFjxpiH/304AS8TffLJJyYoKMi4ubmZqlWrmg0bNjie69y5swkJCXHqv379elOlShXj5uZmihQpYj799NMHXDGysrSMp5CQECMp2aNz584PvnBkSWl9ffonAh7+Ka1jad++faZBgwbGw8PDFCpUyPTp08f8/fffD7hqZFVpHU8TJkww5cqVMx4eHqZAgQKmQ4cO5uTJkw+4amQ169atu+P7oIf9fbjNGOaoAQAAAMAK+AweAAAAAFgEAQ8AAAAALIKABwAAAAAWQcADAAAAAIsg4AEAAACARRDwAAAAAMAiCHgAAAAAYBEEPAAAAACwCAIeAAAAAFgEAQ8AgPvkzz//1AsvvKDChQvLbrcrf/78atSokbZu3ZrZpQEALMolswsAAMCqWrVqpfj4eM2cOVPFihXTmTNnFBkZqfPnz9+X412/fl1ubm73Zd8AgIcDM3gAANwHFy9e1ObNm/X+++8rNDRUQUFBql69uvr376+mTZs6+jz//PPKly+f3N3dVaFCBa1YscKxj8WLF6t8+fKy2+0qUqSIxowZ43SMIkWKaPjw4erSpYt8fX3Vo0cPSdKWLVtUt25deXh4KDAwUL1791ZcXNyDO3kAQKYh4AEAcB94e3vL29tbS5cu1bVr15I9n5iYqPDwcG3ZskVz5sxRdHS03nvvPWXPnl2StHPnTrVp00bt2rXTnj17NGTIEA0cOFAzZsxw2s+HH36oChUqaOfOnRo4cKD27NmjRo0aqWXLlvr555+1cOFCbd68Wb169XoQpw0AyGQ2Y4zJ7CIAALCixYsXq0ePHrpy5YqqVq2qkJAQtWvXTsHBwVq9erXCw8O1b98+lSpVKtm2HTp00NmzZ7V69WpHW79+/fTNN9/ol19+kXRzBq9KlSpasmSJo0+nTp3k4eGhzz77zNG2efNmhYSEKC4uTu7u7vfxjAEAmY0ZPAAA7pNWrVrp1KlTWrZsmRo1aqT169eratWqmjFjhqKiolSoUKEUw50k7du3T3Xq1HFqq1Onjg4ePKgbN2442qpVq+bUZ+fOnZoxY4ZjBtHb21uNGjVSYmKijhw5kvEnCQDIUlhkBQCA+8jd3V0NGzZUw4YNNWjQIHXv3l2DBw9W375977idMUY2my1Z2628vLycfk5MTNQLL7yg3r17J+tbuHDhdJwBAOBhQsADAOABKleunJYuXarg4GCdPHlSBw4cSHEWr1y5ctq8ebNT25YtW1SqVCnH5/RSUrVqVf3yyy8qUaJEhtcOAMj6uEUTAID74Ny5c6pfv77mzJmjn3/+WUeOHNGiRYv0wQcfqHnz5goJCVHdunXVqlUrrVmzRkeOHNG3336r7777TpL0xhtvKDIyUu+++64OHDigmTNn6uOPP77rzN9bb72lrVu36uWXX1ZUVJQOHjyoZcuW6ZVXXnkQpw0AyGTM4AEAcB94e3urRo0aGjdunA4fPqz4+HgFBgaqR48eeueddyTdXISlb9++at++veLi4lSiRAm99957km7OxH355ZcaNGiQ3n33XRUoUEDDhg1Tly5d7njc4OBgbdiwQQMGDNDjjz8uY4yKFy+utm3b3u9TBgBkAayiCQAAAAAWwS2aAAAAAGARBDwAAAAAsAgCHgAAAABYBAEPAAAAACyCgAcAAAAAFkHAAwAAAACLIOABAAAAgEUQ8AAAAADAIgh4AAAAAGARBDwAAAAAsAgCHgAAAABYxP8DPfYXTq0lSogAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "##1st student name: Philip Acquaye-Mensah\n",
        "# Import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the schema for the DataFrame explicitly for Logistic Regression\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Model Metrics Logistic Regression\").getOrCreate()\n",
        "schema_lr = StructType([\n",
        "    StructField(\"Metric\", StringType(), True),\n",
        "    StructField(\"Score\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Assuming 'metrics' contains float64 types, convert them to float (Python native type)\n",
        "metrics_lr = [float(accuracy_lr), float(precision_lr), float(recall_lr), float(roc_auc_lr), float(f1_lr)]\n",
        "metric_names_lr = [\"Accuracy\", \"Precision\", \"Recall\", \"ROC AUC\", \"F1 Score\"]\n",
        "results_lr = list(zip(metric_names_lr, metrics_lr))\n",
        "\n",
        "# Create DataFrame using the defined schema for Logistic Regression\n",
        "results_df_lr = spark.createDataFrame(results_lr, schema_lr)\n",
        "\n",
        "# Convert to Pandas DataFrame for visualization\n",
        "results_df_pd_lr = results_df_lr.toPandas()\n",
        "\n",
        "# Plot the results for Logistic Regression\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=\"Score\", y=\"Metric\", data=results_df_pd_lr, palette=\"viridis\")\n",
        "plt.title(\"Logistic Regression Model Performance Metrics\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Metric\")\n",
        "plt.show()\n",
        "\n",
        "# Plot the confusion for Logistic Regression\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(ConfusionMatrix, annot=True, fmt=\"d\", cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix for Logistic Regression')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 2nd student name: Mohamed Jareer Mohamed Zeenam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define evaluation metrics and values\n",
        "metrics = ['Accuracy', 'F1 Score', 'Precision', 'Recall', 'ROC AUC']\n",
        "values = [acc, f1, precision, recall, auc]\n",
        "\n",
        "# Define colors for the bars\n",
        "colors = ['skyblue', 'lightgreen', 'salmon', 'gold', 'lightcoral']\n",
        "\n",
        "# Create a bar plot with custom styling\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(metrics, values, color=colors)\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Evaluation Metrics')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Values')\n",
        "\n",
        "# Add text labels on top of the bars\n",
        "for bar, value in zip(bars, values):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),  f'{round(value * 100, 2)}%',\n",
        "             ha='center', va='bottom')\n",
        "\n",
        "# Show plot\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/17 01:30:15 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 143580 ms exceeds timeout 120000 ms\n",
            "24/04/17 01:30:15 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
            "24/04/17 01:30:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:30:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:30:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:30:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:30:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:30:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:30:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:30:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:30:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/17 01:30:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/17 01:31:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:31:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:31:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:31:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:31:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:31:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:31:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:31:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:31:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:31:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:31:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/17 01:31:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/17 01:32:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:32:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:32:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:32:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:32:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:32:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:32:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:32:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:32:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:32:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:32:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:32:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:33:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:33:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:33:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:33:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:33:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:33:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:33:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:33:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:33:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/17 01:33:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@philips-mbp:50700\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n"
          ]
        }
      ],
      "source": [
        "# 2nd student name: Mohamed Jareer Mohamed Zeenam\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Convert confusion matrix to pandas DataFrame\n",
        "confusion_matrix_pd = confusion_matrix.toPandas()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix_pd, annot=True, cmap='Reds', fmt='g')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIGDXqDdIQ4U"
      },
      "source": [
        "---\n",
        "# **Task 6 - LSEP Considerations (5 marks)**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuCxxQnYI_FP"
      },
      "source": [
        "# Student 1: **Bias and Fairness**\n",
        "\n",
        "Issue: Models trained on network traffic data might inadvertently learn to identify benign activities as malicious (false positives) or vice versa (false negatives), particularly if the training data is not representative of all types of network behaviors. \n",
        "\n",
        "Mitigation: Utilise a diverse dataset that covers various types of network behaviors to train the model. Regularly validate the model against a diverse test set to ensure that it does not exhibit biased behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgLfrAGfJQxF"
      },
      "source": [
        "# Student 2: **Type the chosen issue**\n",
        "\n",
        "add contribution here ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIM6uLApSxi2"
      },
      "source": [
        "---\n",
        "\n",
        "# **Task 7 - Convert ipynb to HTML for Turnitin submission [5 marks]**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrQu11N_DCfZ"
      },
      "outputs": [],
      "source": [
        "# install nbconvert (if facing the conversion error)\n",
        "#!pip3 install nbconvert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReZWnCYdDH66"
      },
      "outputs": [],
      "source": [
        "# convert ipynb to html and submit this HTML file\n",
        "#!jupyter nbconvert --to html Group_ID_13_CRWK_CN7030.ipynb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
